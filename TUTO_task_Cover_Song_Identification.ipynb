{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-EnXzB1-r9H"
      },
      "source": [
        "# Tutorial Cover-Song-Identification based on Triplet-Loss\n",
        "\n",
        "This notebook was created for the tutorial held at ISMIR-2024 \"Deep-Learning 101 for Audio-based Music Information Retrieval\".\n",
        "If you use this notebook, please cite\n",
        "```\n",
        "@book{deeplearning-101-audiomir:book,\n",
        "\tauthor = {Peeters, Geoffroy and Meseguer-Brocal, Gabriel and Riou, Alain and Lattner, Stefan},\n",
        "\ttitle = {Deep Learning 101 for Audio-based MIR, ISMIR 2024 Tutorial},\n",
        "\turl = {https://geoffroypeeters.github.io/deeplearning-101-audiomir_book},\n",
        "\taddress = {San Francisco, USA},\n",
        "\tyear = {2024},\n",
        "\tmonth = November,\n",
        "\tdoi = {???},\n",
        "}\n",
        "```\n",
        "\n",
        "It illustrates the use of various deep-learning bricks to solve the task of \"Cover-Song-Identification\".\n",
        "\n",
        "Part of the code is based on https://github.com/furkanyesiler/mov for the MOVE model and on https://gist.github.com/bwhite/3726239 for evaluation.\n",
        "Datasets are available at https://www.covers1000.net/dataset.html for Cover-1000 and at https://github.com/MTG/da-tacos for DA-TACOS ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tPHF2mD-r9J"
      },
      "source": [
        "## Deployment\n",
        "\n",
        "In case the notebook is run on GoogleColab or others, we first need to\n",
        "- get the packages: `git clone``\n",
        "- get the datasets\n",
        "\n",
        "We also test the models on two different datasets:\n",
        "- cover-1000\n",
        "- Da-tacos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "NcVMn7TX-r9J",
        "outputId": "5746774a-1970-45ed-c438-a129bbaa1c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deeplearning-101-audiomir_notebook'...\n",
            "remote: Enumerating objects: 257, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 257 (delta 58), reused 56 (delta 26), pack-reused 146 (from 1)\u001b[K\n",
            "Receiving objects: 100% (257/257), 65.63 MiB | 31.49 MiB/s, done.\n",
            "Resolving deltas: 100% (146/146), done.\n",
            "/content/deeplearning-101-audiomir_notebook\n",
            "config_autotagging.yaml\n",
            "config_bittner.yaml\n",
            "config_chord.yaml\n",
            "config_cover.yaml\n",
            "config_doras.yaml\n",
            "feature.py\n",
            "model_factory.py\n",
            "model_module.py\n",
            "README.md\n",
            "TUTO_task_Auto_Tagging.ipynb\n",
            "TUTO_task_Auto_Tagging.ipynb_D1-I1-C1.ipynb\n",
            "TUTO_task_Auto_Tagging.ipynb_D1-I2-C2.ipynb\n",
            "TUTO_task_Auto_Tagging.ipynb_D1-I2-C3.ipynb\n",
            "TUTO_task_Auto_Tagging.ipynb_D1-I2-C4.ipynb\n",
            "TUTO_task_Auto_Tagging.ipynb_D2-I1-C1.ipynb\n",
            "TUTO_task_Auto_Tagging.ipynb_D3-I3-Chord.ipynb\n",
            "TUTO_task_Cover_Song_Identification.ipynb\n",
            "TUTO_task_Cover_Song_Identification.ipynb_cover1000.ipynb\n",
            "TUTO_task_Cover_Song_Identification.ipynb_datacos.ipynb\n",
            "TUTO_task_Generation_Autoregressive.ipynb\n",
            "TUTO_task_Generation_Diffusion.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D1-I1-C1.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D1-I2-C1.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D1-I2-C2.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D1-I2-C3.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D1-I2-C4.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D1-I2-Unet.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D2-I2-C1.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D2-I2-C3.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D2-I2-Unet.ipynb\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "munchify() missing 1 required positional argument: 'x'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-df1851fe50c1>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mcfg_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmunchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: munchify() missing 1 required positional argument: 'x'"
          ]
        }
      ],
      "source": [
        "do_deploy = True\n",
        "\n",
        "if do_deploy:\n",
        "    !git clone https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook.git\n",
        "    %cd /content/deeplearning-101-audiomir_notebook\n",
        "    !ls\n",
        "\n",
        "    import urllib.request\n",
        "    import shutil\n",
        "    ROOT = 'https://perso.telecom-paristech.fr/gpeeters/tuto_DL101forMIR/'\n",
        "\n",
        "    hdf5_feat_file, pyjama_annot_file = 'cover1000_feat.hdf5.zip', 'cover1000.pyjama'\n",
        "    #hdf5_feat_file, pyjama_annot_file = 'datacos-benchmark_feat.hdf5.zip', 'datacos-benchmark.pyjama'\n",
        "\n",
        "    urllib.request.urlretrieve(ROOT + hdf5_feat_file, hdf5_feat_file)\n",
        "    if hdf5_feat_file.endswith('.zip'): shutil.unpack_archive(hdf5_feat_file, './')\n",
        "    urllib.request.urlretrieve(ROOT + pyjama_annot_file, pyjama_annot_file)\n",
        "\n",
        "    ROOT = './'\n",
        "\n",
        "else:\n",
        "\n",
        "    ROOT = '/tsi/data_doctorants/gpeeters/_data/'\n",
        "\n",
        "config_file = 'config_cover.yaml'\n",
        "\n",
        "do_wandb = False\n",
        "import pprint as pp\n",
        "import yaml\n",
        "! pip install munch --quiet\n",
        "from munch import munchify\n",
        "with open(config_file, 'r') as fid:\n",
        "    cfg_dic = yaml.safe_load(fid)\n",
        "config = munchify(cfg_dic)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pp.pprint(cfg_dic)"
      ],
      "metadata": {
        "id": "w7Aw3OI--w7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grJ7M3Cz-r9L"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEtV0sAi-r9L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "#import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "import torchsummary\n",
        "\n",
        "! pip install lightning --quiet\n",
        "import lightning.pytorch as pl\n",
        "from lightning.pytorch.callbacks import Callback\n",
        "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "! pip install wandb --quiet\n",
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "import json\n",
        "import yaml\n",
        "import h5py\n",
        "import pprint as pp\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "\n",
        "\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from argparse import Namespace\n",
        "\n",
        "# -----------------------------\n",
        "import model_factory\n",
        "import importlib\n",
        "importlib.reload(model_factory)\n",
        "\n",
        "from argparse import Namespace\n",
        "! pip install munch --quiet\n",
        "from munch import munchify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6owpzyuD-r9L"
      },
      "source": [
        "## Set fixed seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pzrFEi4-r9M"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "seed = 42\n",
        "torch.manual_seed(seed)         # For CPU\n",
        "random.seed(seed)               # For Python's built-in random module\n",
        "np.random.seed(seed)            # For NumPy\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)          # For current GPU\n",
        "    torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVMTnqeA-r9M"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFvhMZWk-r9M"
      },
      "outputs": [],
      "source": [
        "hdf5_feat_file = f'{ROOT}/{config.dataset.base}_feat.hdf5'\n",
        "pyjama_annot_file = f'{ROOT}/{config.dataset.base}.pyjama'\n",
        "\n",
        "param_model = Namespace()\n",
        "param_model.num_of_labels = 16\n",
        "param_model.emb_size = 32   #16000\n",
        "param_model.sum_method = 4\n",
        "param_model.final_activation = 3\n",
        "param_model.downsampling_parameters = 2\n",
        "\n",
        "param_model.margin =  1.0\n",
        "param_model.mining_strategy = 1\n",
        "param_model.norm_dist = 1\n",
        "param_model.lr = 0.1\n",
        "param_model.momentum = 0\n",
        "param_model.nb_epoch = 100\n",
        "\n",
        "param_model.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "param_lightning = Namespace()\n",
        "param_lightning.max_epochs = 500\n",
        "param_lightning.dirpath='./_cover_lighning/'\n",
        "param_lightning.filename='best_model_cover'\n",
        "\n",
        "param_wandb = Namespace()\n",
        "param_wandb.save_dir = './_cover_wandb/'\n",
        "param_wandb.project_name = 'wandb_cover'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIrCs-lE-r9M"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxO5cly1-r9N"
      },
      "source": [
        "### Test loading pyjama/hdf5\n",
        "\n",
        "All the audio data of a dataset are stored in a single [.hdf5](https://docs.h5py.org/) file.\n",
        "Each `key` corresponds to an entry, a specific audiofile.\n",
        "Its array contains the audio features (here the CREMA features matrix).\n",
        "\n",
        "All the annotations of a dataset are stored in a single *.pyjama file.\n",
        "As [JAMS](https://github.com/marl/jams) files, .pyjama files are JSON files.\n",
        "However, a single .pyjama file can contain the annotations of ALL entries of a dataset.\n",
        "Its specifications are described here [DOC](https://github.com/geoffroypeeters/pyjama).\n",
        "The values of the `filepath` field of the .pyjama file correspond to the `key` values of the .hdf5 file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM72oJdT-r9N"
      },
      "outputs": [],
      "source": [
        "with open(pyjama_annot_file, encoding = \"utf-8\") as json_fid:\n",
        "    data_d = json.load(json_fid)\n",
        "entry_l = data_d['collection']['entry']\n",
        "\n",
        "# --- example of entry (for illustration)\n",
        "pp.pprint(entry_l[0:2])\n",
        "\n",
        "# --- get list of audio files\n",
        "audiofile_l = [entry['filepath'][0]['value'] for entry in entry_l]\n",
        "print(f'number of audio: {len(audiofile_l)}')\n",
        "# --- example of audio (for illustration)\n",
        "pp.pprint(audiofile_l[:5])\n",
        "\n",
        "with h5py.File(hdf5_feat_file, 'r') as hdf5_fid:\n",
        "    #audiofile_l = [key for key in hdf5_fid['/'].keys()]\n",
        "    pp.pprint(f\"feature shape: {hdf5_fid[audiofile_l[0]][:].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDHf6cs7-r9N"
      },
      "source": [
        "We check how many entry are contained in the dataset, how many unique `performance-id` and how many unique `work-id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUNFzYY9-r9N"
      },
      "outputs": [],
      "source": [
        "with open(pyjama_annot_file, encoding = \"utf-8\") as json_fid:\n",
        "    data_d = json.load(json_fid)\n",
        "entry_l = data_d['collection']['entry']\n",
        "performanceid_l  = set([entry['performance-id'][0]['value'] for entry in entry_l])\n",
        "workid_l  = set([entry['work-id'][0]['value'] for entry in entry_l])\n",
        "\n",
        "print(f'number of tracks {len(entry_l)}')\n",
        "print(f'number of performance {len(performanceid_l)}')\n",
        "print(f'number of work {len(workid_l)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuPtiz7i-r9N"
      },
      "source": [
        "### Create Dataset\n",
        "\n",
        "The class `CoverDataset` (a subset of pytorch `Dataset` class) is responsible for providing (with the `__getitem` method) four performances (performance-id) that belongs to the same work-id.\n",
        "\n",
        "- `getitem_by_performanceid`: will truncate/pad the feature to achieve a length of 1800\n",
        "- `__getitem__`: for a given work-id, it will randomy select a subset of 4 performances that belong to it\n",
        "- '__init__':\n",
        "  - will read the hdf5_feat_file and pyjama_annot_file\n",
        "  - split their content according to training and test (selection ensure that work-id are different in train and valid)\n",
        "  - create a set of mapping dictionary\n",
        "  - create a list of cliques: a clique groups performance corresponding to the same workid. Here, if one workid has many performanceid we duplicate the correspond clique so that it appears more often in the `__getitem__` method\n",
        "  - store all the necessary data in memory (of CPU) to fasten later access: `self.data_d[key=performanceid]`. Part of the data are the feature. We map the 12-dimension CREMA features to 23-dimension by concatenating them with their transpose version. This is specific to the MOVE model which first layer try to obtain a pitch-shift invariance with the `[Conv2d, {'in_channels':-1, 'out_channels':256, 'kernel_size':[12, 180], 'stride':[1,1]}]` and `[MaxPool2d, {'kernel_size':[12, 1], 'stride': [1, 1]}]` layers.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm7lZsez-r9N"
      },
      "outputs": [],
      "source": [
        "class CoverDataset(Dataset):\n",
        "    \"\"\"\n",
        "    description\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hdf5_feat_file, pyjama_annot_file, do_train):\n",
        "\n",
        "        self.h = 23\n",
        "        self.w = 1800\n",
        "\n",
        "        with open(pyjama_annot_file, encoding = \"utf-8\") as json_fid: data_d = json.load(json_fid)\n",
        "        entry_l = data_d['collection']['entry']\n",
        "\n",
        "        all_workid_l  = list(set([entry['work-id'][0]['value'] for entry in entry_l]))\n",
        "        performanceid_l  = set([entry['performance-id'][0]['value'] for entry in entry_l])\n",
        "\n",
        "        self.do_train = do_train\n",
        "        if self.do_train:   workid_l = [all_workid_l[idx] for idx in range(len(all_workid_l)) if (idx % 5) != 0]\n",
        "        else:               workid_l = [all_workid_l[idx] for idx in range(len(all_workid_l)) if (idx % 5) == 0]\n",
        "\n",
        "        self.workid_to_perfomanceid_d = {}\n",
        "        for workid in workid_l:\n",
        "            self.workid_to_perfomanceid_d[workid] = [entry['performance-id'][0]['value'] for entry in entry_l if entry['work-id'][0]['value']==workid]\n",
        "\n",
        "        # --- each element of the self.clique_list_l correspond to a single work-id\n",
        "        # --- but if a work-id has many performance-id we create several self.clique_list_l for it\n",
        "        # --- such that it appears more often during the __getitem__\n",
        "        self.clique_list_l = []\n",
        "        for workid in self.workid_to_perfomanceid_d.keys():\n",
        "            # --- for each workid, we check the number of performanceid\n",
        "            # --- if this number is large this workid will be present in many clique\n",
        "            nb_performanceid = len(self.workid_to_perfomanceid_d[workid])\n",
        "            #print(len(self.clique_list_l), workid, nb_performanceid)\n",
        "            if nb_performanceid < 2:     pass\n",
        "            elif nb_performanceid < 6:   self.clique_list_l.extend([workid] * 1)\n",
        "            elif nb_performanceid < 10:  self.clique_list_l.extend([workid] * 2)\n",
        "            elif nb_performanceid < 14:  self.clique_list_l.extend([workid] * 3)\n",
        "            else:                        self.clique_list_l.extend([workid] * 4)\n",
        "\n",
        "        self.data_d = {}\n",
        "        with h5py.File(hdf5_feat_file, 'r') as feat_fid:\n",
        "            for workid in self.workid_to_perfomanceid_d.keys():\n",
        "                # --- get the list of performanceid associated to this workid\n",
        "                perfomanceid_l = self.workid_to_perfomanceid_d[workid]\n",
        "                for performanceid in perfomanceid_l:\n",
        "                    self.data_d[performanceid] = {}\n",
        "                    self.data_d[performanceid]['workid'] = workid\n",
        "                    self.data_d[performanceid]['perfomanceid'] = performanceid\n",
        "                    # --- Get data and convert to make rotation invariant -> self.data_d (1, 23, 1800)\n",
        "                    data = torch.from_numpy(feat_fid['/' +  str(performanceid) + '/'][:].T).unsqueeze(0)\n",
        "                    self.data_d[performanceid]['X'] = torch.concatenate((data, data), dim=1)[:,:-1,:]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clique_list_l)\n",
        "\n",
        "\n",
        "    def getitem_by_performanceid(self, perfomanceid):\n",
        "        X = self.data_d[perfomanceid]['X']\n",
        "        # if the song is longer than the required width, choose a random start point to crop\n",
        "        if X.shape[2] >= self.w:\n",
        "            X = X[:, :, 0:self.w]\n",
        "        else:\n",
        "            X = torch.cat((X, torch.zeros([1, self.h, self.w - X.shape[2]])), 2)\n",
        "        return X, self.data_d[perfomanceid]['workid']\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx_clique):\n",
        "\n",
        "        workid = self.clique_list_l[idx_clique]  # getting the clique chosen by the dataloader\n",
        "\n",
        "        # selecting 4 songs from the given clique\n",
        "        if len(self.workid_to_perfomanceid_d[workid]) == 2:  # if the clique size is 2, repeat the already selected songs\n",
        "            performanceid_1, performanceid_2 = np.random.choice(self.workid_to_perfomanceid_d[workid], 2, replace=False)\n",
        "            performanceid_3, performanceid_4 = performanceid_1, performanceid_2\n",
        "        elif len(self.workid_to_perfomanceid_d[workid]) == 3:  # if the clique size is 3, choose one of the songs twice\n",
        "            performanceid_1, performanceid_2, performanceid_3 = np.random.choice(self.workid_to_perfomanceid_d[workid], 3, replace=False)\n",
        "            performanceid_4 = np.random.choice(self.workid_to_perfomanceid_d[workid], 1, replace=False)[0]\n",
        "        else:  # if the clique size is larger than or equal to 4, choose 4 songs randomly\n",
        "            performanceid_1, performanceid_2, performanceid_3, performanceid_4 = np.random.choice(self.workid_to_perfomanceid_d[workid], 4, replace=False)\n",
        "\n",
        "        X_l = []\n",
        "        for perfomanceid in [performanceid_1, performanceid_2, performanceid_3, performanceid_4]:\n",
        "            X, _ = self.getitem_by_performanceid(perfomanceid)\n",
        "            X_l.append(X)\n",
        "\n",
        "        return torch.stack(X_l, 0), workid\n",
        "\n",
        "train_dataset = CoverDataset(hdf5_feat_file, pyjama_annot_file, do_train=True)\n",
        "valid_dataset = CoverDataset(hdf5_feat_file, pyjama_annot_file, do_train=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAAoMDXv-r9O"
      },
      "outputs": [],
      "source": [
        "# --- TEST\n",
        "items, label = train_dataset[0]\n",
        "print(items.size())\n",
        "print(label)\n",
        "\n",
        "plt.subplot(211); plt.imshow(items[0,0,:,:].cpu().numpy(), aspect='auto'); plt.colorbar();\n",
        "plt.subplot(212); plt.imshow(items[1,0,:,:].cpu().numpy(), aspect='auto'); plt.colorbar();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrFBl9JK-r9O"
      },
      "source": [
        "### Create DataLoader\n",
        "\n",
        "We create the dataloader for the training and validation data from the corresponding dataset.\n",
        "We use here a specific `collate function`, i.e. the function used to collate the data provided by the `__getitem__` of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRCeLsfo-r9O"
      },
      "outputs": [],
      "source": [
        "def triplet_mining_collate(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for triplet mining\n",
        "    :param batch: elements of the mini-batch (pcp features and labels)\n",
        "    :return: collated elements\n",
        "    \"\"\"\n",
        "    items = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "\n",
        "    return torch.cat(items, 0), labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77JvFH1Y-r9O"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=param_model.num_of_labels, shuffle=True, collate_fn=triplet_mining_collate, num_workers=9, drop_last=True)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=param_model.num_of_labels, shuffle=False, collate_fn=triplet_mining_collate, num_workers=9, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41tgCw41-r9O"
      },
      "outputs": [],
      "source": [
        "# --- TEST\n",
        "items, labels = next(iter(train_dataloader))\n",
        "print(items.size())\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygfq9FZ--r9O"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5V1tsPO-r9P"
      },
      "source": [
        "The pytorch model is generated automatically based on the content of the `config_cover.yaml` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xsjfsp9m-r9P"
      },
      "outputs": [],
      "source": [
        "importlib.reload(model_factory)\n",
        "\n",
        "m, C, H, T = 32, 1, 23, 1800\n",
        "model = model = model_factory.NetModel(config, [m, C, H, T])\n",
        "model = model.to(param_model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU7U6qoE-r9P"
      },
      "source": [
        "### Check the model\n",
        "\n",
        "We can check the model either using\n",
        "- print(model)\n",
        "- the `verbose` mode of the `forward` of the model\n",
        "- the `torchsummary.summary` method\n",
        "\n",
        "The model used has `1,869,093` parameters to be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MECqruCN-r9P"
      },
      "outputs": [],
      "source": [
        "# --- TEST\n",
        "#print(model)\n",
        "\n",
        "X = torch.randn(m, C, H, T).to(param_model.device)\n",
        "print(model(X, True).size())\n",
        "\n",
        "#torchsummary.summary(model, (C, H, T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qce2XE-N-r9P"
      },
      "source": [
        "### Define losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDj9-CbB-r9P"
      },
      "source": [
        "The triplet loss using online triple-mining we use necessitates the writing of specific functions.\n",
        "- `pairwise_distance_matrix`: compute the squared eucliden distance between the embeddings of all data in a batch\n",
        "- `f_renumber`: replicates the single work-id associated to an output of `__getitem__` to the 4 features (hence 4 embeddings) provided by it  \n",
        "- `triplet_loss_mining` : is the main function which\n",
        "  - compute the distances\n",
        "  - compute the positive (performances with same workid) and negative mask (performances with different workid)\n",
        "  - select the correct triplet using various methods (random, semi-hard, hard)\n",
        "  - compute the triplet loss\n",
        "- `triplet_mining_random`, `triplet_mining_semihard`, `triplet_mining_hard` are the corresponding functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZnphvdy-r9P"
      },
      "outputs": [],
      "source": [
        "def pairwise_distance_matrix(x, y=None, eps=1e-12):\n",
        "    \"\"\"\n",
        "    Calculating squared euclidean distances between the elements of two tensors\n",
        "    :param x: first tensor\n",
        "    :param y: second tensor (optional)\n",
        "    :param eps: epsilon value for avoiding div by zero\n",
        "    :return: pairwise distance matrix\n",
        "    \"\"\"\n",
        "    x_norm = x.pow(2).sum(dim=1).view(-1, 1)\n",
        "    if y is not None:\n",
        "        y_norm = y.pow(2).sum(dim=1).view(1, -1)\n",
        "    else:\n",
        "        y = x\n",
        "        y_norm = x_norm.view(1, -1)\n",
        "\n",
        "    dist = x_norm + y_norm - 2 * torch.mm(x, y.t().contiguous())\n",
        "    return torch.clamp(dist, eps, np.inf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWxqmu7O-r9P"
      },
      "outputs": [],
      "source": [
        "def f_renumber(labels):\n",
        "    \"\"\"\n",
        "    renumber the labels (which correspond to work-id) starting from 0 and get 4 of them each time\n",
        "    \"\"\"\n",
        "    aux = {}\n",
        "    i_labels = []\n",
        "    for l in labels:\n",
        "        if l not in aux:\n",
        "            aux[l] = len(aux)\n",
        "        i_labels += [aux[l]]*4\n",
        "    return i_labels\n",
        "\n",
        "#f_renumber([300, 200, 500, 300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC7oVhMS-r9Q"
      },
      "outputs": [],
      "source": [
        "def triplet_loss_mining(embedding_m, labels, param_model):\n",
        "    \"\"\"\n",
        "    Online mining function for selecting the triplets\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = embedding_m.size(0)\n",
        "\n",
        "    # creating positive and negative masks for online mining\n",
        "    i_labels = f_renumber(labels)\n",
        "    i_labels = torch.Tensor(i_labels).view(-1, 1).to(param_model.device)\n",
        "    # --- get a ones matrix with zero on main diagonal (to avoid selecting the anchor itself for positive or negative)\n",
        "    mask_diag = (1 - torch.eye(batch_size)).long().to(param_model.device)\n",
        "    # --- the mask with 1 if same work-id 0 otherwise\n",
        "    sameworkid_mask = (pairwise_distance_matrix(i_labels) < 0.5).long()\n",
        "    # --- same work-id but not the anchor\n",
        "    mask_pos = mask_diag * sameworkid_mask\n",
        "    # --- different work-id and not the anchor\n",
        "    mask_neg = mask_diag * (1 - mask_pos)\n",
        "\n",
        "    # getting the pairwise distance matrix\n",
        "    dist_all = pairwise_distance_matrix(embedding_m)\n",
        "    # normalizing the distances by the embedding size\n",
        "    if param_model.norm_dist == 1:  dist_all /= param_model.emb_size\n",
        "\n",
        "    if param_model.mining_strategy == 0:    dist_pos, dist_neg = triplet_mining_random(dist_all, mask_pos, mask_neg)\n",
        "    elif param_model.mining_strategy == 1:  dist_pos, dist_neg = triplet_mining_semihard(dist_all, mask_pos, mask_neg, param_model.margin)\n",
        "    else:                                   dist_pos, dist_neg = triplet_mining_hard(dist_all, mask_pos, mask_neg, param_model.device)\n",
        "\n",
        "    loss = F.relu(dist_pos + (param_model.margin - dist_neg))  # calculating triplet loss\n",
        "\n",
        "    nb1 = torch.sum(dist_pos+param_model.margin < dist_neg).item()\n",
        "\n",
        "    return loss.mean(), nb1\n",
        "\n",
        "\n",
        "def triplet_mining_random(dist_all, mask_pos, mask_neg):\n",
        "    \"\"\"\n",
        "    Performs online random triplet mining\n",
        "    \"\"\"\n",
        "    # selecting the positive elements of triplets\n",
        "    # we consider each row as an anchor and takes the maximum of the masked row (mask_pos) as the positive\n",
        "    _, sel_pos = torch.max(mask_pos.float() + torch.rand_like(dist_all), dim=1)\n",
        "    dists_pos = torch.gather(input=dist_all, dim=1, index=sel_pos.view(-1, 1))\n",
        "\n",
        "    # selecting the negative elements of triplets\n",
        "    # we consider each row as an anchor and takes the maximum of the masked row (mask_neg) as the negative\n",
        "    _, sel_neg = torch.max(mask_neg.float() + torch.rand_like(dist_all), dim=1)\n",
        "    dists_neg = torch.gather(input=dist_all, dim=1, index=sel_neg.view(-1, 1))\n",
        "\n",
        "    return dists_pos, dists_neg\n",
        "\n",
        "\n",
        "def triplet_mining_semihard(dist_all, mask_pos, mask_neg, margin):\n",
        "    \"\"\"\n",
        "    Performs online semi-hard triplet mining (a random positive, a semi-hard negative)\n",
        "    \"\"\"\n",
        "\n",
        "    # --- the code below seems wrong\n",
        "    # --- need criteria\n",
        "    # 1) should be negative (should be from a different work-id)\n",
        "    # 2) should be P < N < P+margin\n",
        "\n",
        "    # selecting the positive elements of triplets\n",
        "    # we consider each row as an anchor and takes the maximum of the masked row (mask_pos) as the positive\n",
        "    _, sel_pos = torch.max(mask_pos.float() + torch.rand_like(dist_all), dim=1)\n",
        "    dists_pos = torch.gather(input=dist_all, dim=1, index=sel_pos.view(-1, 1))\n",
        "\n",
        "    # selecting the negative elements of triplets\n",
        "    _, sel_neg = torch.max(\n",
        "                            (mask_neg + mask_neg * (dist_all < (dists_pos.expand_as(dist_all)).long()+margin)).float()\n",
        "                           + torch.rand_like(dist_all),\n",
        "                           dim=1)\n",
        "\n",
        "    dists_neg = torch.gather(input=dist_all, dim=1, index=sel_neg.view(-1, 1))\n",
        "\n",
        "    return dists_pos, dists_neg\n",
        "\n",
        "\n",
        "def triplet_mining_hard(dist_all, mask_pos, mask_neg, device):\n",
        "    \"\"\"\n",
        "    Performs online hard triplet mining (both positive and negative)\n",
        "    \"\"\"\n",
        "\n",
        "    # --- the code below seems wrong\n",
        "    # --- need criteria\n",
        "    # 1) should be negative (from a different work-id)\n",
        "    # 2) should be N < P\n",
        "\n",
        "    # selecting the positive elements of triplets\n",
        "    # --- for each anchor (row) we take the positive with the largest distance\n",
        "    _, sel_pos = torch.max(dist_all * mask_pos.float(), 1)\n",
        "    dists_pos = torch.gather(input=dist_all, dim=1, index=sel_pos.view(-1, 1))\n",
        "\n",
        "    # modifying the negative mask for hard mining (because we will use the min)\n",
        "    # --- if mask_neg==0 then inf\n",
        "    # --- if mask_neg==1 then 1\n",
        "    true_value = torch.tensor(float('inf'), device=device)\n",
        "    false_value = torch.tensor(1., device=device)\n",
        "    mask_neg = torch.where(mask_neg == 0, true_value, false_value)\n",
        "    # selecting the negative elements of triplets\n",
        "    # --- for each anchor (row) we take the negative with the smallest distance\n",
        "    _, sel_neg = torch.min(dist_all + mask_neg.float(), dim=1)\n",
        "    dists_neg = torch.gather(input=dist_all, dim=1, index=sel_neg.view(-1, 1))\n",
        "\n",
        "    return dists_pos, dists_neg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiVNsIAu-r9Q"
      },
      "source": [
        "## Training using TorchLightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKHVJXbi-r9Q"
      },
      "source": [
        "### W&B configuration\n",
        "\n",
        "We configure `https://wandb.ai/` to monitor the training of our model.\n",
        "As `tensorboard`, `wandb` is a server that keep track of the performance in real-time of a raining.\n",
        "Unlike `tensorboard`, `wandb` server runs online and can therefore be accessed from anywhere (you can monitor your training in the bus, train, metro)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MopZxvfo-r9Q"
      },
      "outputs": [],
      "source": [
        "if do_wandb:\n",
        "    train_config_d = {}\n",
        "    current_datetime = datetime.datetime.now()\n",
        "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "    expe_name = formatted_datetime\n",
        "    wandb.finish()\n",
        "    wandb_logger = WandbLogger(project = param_wandb.project_name, name = expe_name, save_dir = param_wandb.save_dir )\n",
        "    wandb_logger.experiment.config.update(train_config_d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyt2Qz7b-r9Q"
      },
      "source": [
        "### TorchLightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhTfh59I-r9Q"
      },
      "outputs": [],
      "source": [
        "class CoverLigthing(pl.LightningModule):\n",
        "    def __init__(self, in_model):\n",
        "        super().__init__()\n",
        "        self.model = in_model\n",
        "        self.loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        items, labels = batch\n",
        "        embedding_m = self.model(items.to(param_model.device))\n",
        "        loss, oktriplet = triplet_loss_mining(embedding_m, labels, param_model)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.log(\"train_oktriplet\", oktriplet, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        items, labels = batch\n",
        "        embedding_m = self.model(items.to(param_model.device))\n",
        "        loss, oktriplet = triplet_loss_mining(embedding_m, labels, param_model)\n",
        "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
        "        self.log(\"valid_oktriplet\", oktriplet, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.SGD(self.parameters(), lr=param_model.lr, momentum=param_model.momentum)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IE4ZJBf-r9Q"
      },
      "source": [
        "### Training\n",
        "\n",
        "We train the model and apply a early-stopping based on the validation loss.\n",
        "We also monitor the number of triplets that are OK (i.e. $d(A,P)+\\alpha < d(A,N)$) for the training and validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOaW7t0c-r9R"
      },
      "outputs": [],
      "source": [
        "my_lighting = CoverLigthing(model)\n",
        "early_stop_callback = EarlyStopping(monitor=\"valid_loss\", patience=50, verbose=True, mode=\"min\")\n",
        "checkpoint_callback = ModelCheckpoint(monitor='valid_loss', dirpath=param_lightning.dirpath, filename=param_lightning.filename, save_top_k=1, mode='min')\n",
        "if do_wandb:\n",
        "    trainer = pl.Trainer(accelerator=\"gpu\",  logger = wandb_logger, max_epochs = param_lightning.max_epochs, callbacks = [early_stop_callback, checkpoint_callback])\n",
        "else:\n",
        "    trainer = pl.Trainer(accelerator=\"gpu\",  max_epochs = param_lightning.max_epochs, callbacks = [early_stop_callback, checkpoint_callback])\n",
        "trainer.fit(model=my_lighting, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvAG4nb9-r9R"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We first load the best model obtained during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKUYvUu6-r9R"
      },
      "outputs": [],
      "source": [
        "best_model_path = checkpoint_callback.best_model_path\n",
        "#best_model_path = 'my_model/best_model.ckpt'\n",
        "print(f\"Best model saved at: {best_model_path}\")\n",
        "my_lighting = CoverLigthing.load_from_checkpoint(best_model_path, in_model=model)\n",
        "\n",
        "print( type(model) )\n",
        "print( type(my_lighting) )\n",
        "print( type(my_lighting.model) )\n",
        "\n",
        "model = my_lighting.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HazpTqj5-r9R"
      },
      "source": [
        "We send all the data of the validation dataset to the model, get their embedding and compute the distance matrix between all items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEBdSrRg-r9R"
      },
      "outputs": [],
      "source": [
        "model.to(param_model.device)\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    embed_all_m = torch.tensor([], device=param_model.device)\n",
        "    coverid_l = []\n",
        "    for performanceid in valid_dataset.data_d.keys():\n",
        "        items, coverid = valid_dataset.getitem_by_performanceid(performanceid)\n",
        "        embedding_m = model(items.unsqueeze(0).to(param_model.device))\n",
        "        embed_all_m = torch.cat((embed_all_m, embedding_m), dim=0)\n",
        "        coverid_l.append(coverid)\n",
        "\n",
        "dist_all_m = pairwise_distance_matrix(embed_all_m)\n",
        "if param_model.norm_dist == 1:  dist_all_m /= param_model.emb_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulAuka26-r9S"
      },
      "source": [
        "#### Compute Ranking Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivt1S-bR-r9S"
      },
      "outputs": [],
      "source": [
        "def F_mean_rank(relevance):\n",
        "    return relevance.nonzero()[0][0]+1\n",
        "\n",
        "def F_mean_reciprocal_rank(relevance):\n",
        "    return 1./ F_mean_rank(relevance)\n",
        "\n",
        "def F_precision_at_k(relevance, k):\n",
        "    return np.mean(relevance[:k] != 0)\n",
        "\n",
        "def F_average_precision(relevance):\n",
        "    out = [F_precision_at_k(relevance, k + 1) for k in range(relevance.size) if relevance[k]]\n",
        "    return np.mean(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZWDx0Ze-r9S"
      },
      "source": [
        "For each item of the validation dataset (one row of the distance matrix), we compute the mean-rank, mean-reciprocal-rank, precision@1, @5 @10 and mean Average Precision (mAP).\n",
        "We average over all items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fQvtoOG-r9S"
      },
      "outputs": [],
      "source": [
        "dist_all_np = dist_all_m.cpu().numpy()\n",
        "nb_target = dist_all_np.shape[0]\n",
        "dist_all_np += 1e6*np.eye(nb_target) # --- to prevent detecting the target itself\n",
        "mean_rank_l, mean_reciprocal_rank_l, precision_at_1_l, precision_at_5_l, precision_at_10_l, average_precision_l = [], [], [], [], [], []\n",
        "\n",
        "score_d = {}\n",
        "for key in ['mean_rank', 'mean_reciprocal_rank', 'precision_at_1','precision_at_5', 'precision_at_10', 'average_precision']:\n",
        "    score_d[key] = []\n",
        "\n",
        "for idx_target in range(nb_target):\n",
        "    relevance = np.asarray([1 if coverid_l[pos]==coverid_l[idx_target] else 0 for pos in np.argsort(dist_all_np[idx_target,:])])\n",
        "    score_d['mean_rank'].append( F_mean_rank(relevance) )\n",
        "    score_d['mean_reciprocal_rank'].append( F_mean_reciprocal_rank(relevance) )\n",
        "    score_d['precision_at_1'].append( F_precision_at_k(relevance, 1) )\n",
        "    score_d['precision_at_5'].append( F_precision_at_k(relevance, 5) )\n",
        "    score_d['precision_at_10'].append( F_precision_at_k(relevance, 10) )\n",
        "    score_d['average_precision'].append( F_precision_at_k(relevance, 10) )\n",
        "\n",
        "for key in score_d.keys():\n",
        "    print(f\"{key}: {np.mean(np.asarray(score_d[key]))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt0AFALx-r9S"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.copy(config.output_file.origin, config.output_file.origin + config.output_file.ext)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}