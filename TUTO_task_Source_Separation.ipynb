{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuO1F0oEaED9"
   },
   "source": [
    "# Tutorial about Source separation and introducing conditioning\n",
    "\n",
    "- date: 2024-10-10\n",
    "- author: gmeseguerbrocal@deezer.com\n",
    "\n",
    "The code of the model is based on UNet and CUNet for source separation\n",
    "- Papers:\n",
    "    - https://openaccess.city.ac.uk/id/eprint/19289/1/7bb8d1600fba70dd79408775cd0c37a4ff62.pdf\n",
    "    - https://arxiv.org/pdf/1907.01277"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3r0_JSwaED_"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36215,
     "status": "ok",
     "timestamp": 1729764345715,
     "user": {
      "displayName": "Gabriel Meseguer Brocal",
      "userId": "00324772212969630818"
     },
     "user_tz": -120
    },
    "id": "6YAyVnevaED_",
    "outputId": "5012e5f6-138c-49ed-e245-c0217e0c318a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import requests\n",
    "import shutil\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Any, Dict, List, Tuple, Union, Iterator\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "!pip install musdb --quiet\n",
    "import musdb\n",
    "\n",
    "!pip install museval --quiet\n",
    "from museval.metrics import bss_eval\n",
    "\n",
    "!pip install einops --quiet\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BmVyW6daEEA"
   },
   "source": [
    "\n",
    "## Dataset\n",
    "\n",
    "This section of the code will help you:\n",
    "\n",
    "* Download the **MUSDB** dataset: A rich collection of music tracks with mixtures separated into 4 distinct stems: vocals, drums, bass, and others.\n",
    "\n",
    "* Create an Infinite DataLoader: This DataLoader is designed to continuously provide data points, ensuring a seamless supply of both mixed audio and target sources when calling next. Here's how it works:\n",
    "\n",
    "    * **MusDBDataset**: Loads the audio files of the desired split in parallel, accessing all the stems. It chunks each song into individual segments and yields them.\n",
    "\n",
    "    * **SourceSeparationDataloader**: Takes individual segments from MusDBDataset and creates a buffer to mix segments from different audio tracks, ensuring a diverse and continuous stream of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249774,
     "status": "ok",
     "timestamp": 1729764595486,
     "user": {
      "displayName": "Gabriel Meseguer Brocal",
      "userId": "00324772212969630818"
     },
     "user_tz": -120
    },
    "id": "ZU2KbTlOaEEA",
    "outputId": "06c3fc93-2a17-490e-935e-137edddaf810"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.47kMB [02:45, 26.9MB/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete!\n",
      "Extracting /content/musdb/musdb.zip to /content/musdb/...\n"
     ]
    }
   ],
   "source": [
    "FULL_MUSDB = True  # Set this accordingly\n",
    "MUSDB_PATH = '/content/musdb/'\n",
    "\n",
    "if not os.path.exists(MUSDB_PATH):\n",
    "    os.makedirs(MUSDB_PATH)\n",
    "\n",
    "def download_file(url: str, dest_path: str):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024 * 1024  # 1 MB\n",
    "\n",
    "    with open(dest_path, 'wb') as file:\n",
    "        for chunk in tqdm(response.iter_content(chunk_size=block_size), total=total_size // block_size, unit='MB', unit_scale=True):\n",
    "            file.write(chunk)\n",
    "    print(\"\\nDownload complete!\")\n",
    "\n",
    "\n",
    "def extract_zip(zip_path: str, extract_to: str):\n",
    "    print(f\"Extracting {zip_path} to {extract_to}...\")\n",
    "    shutil.unpack_archive(zip_path, extract_to)\n",
    "\n",
    "def remove_dir(path: str):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "if FULL_MUSDB:\n",
    "    download_file('https://zenodo.org/records/1117372/files/musdb18.zip?download=1', os.path.join(MUSDB_PATH, 'musdb.zip'))\n",
    "\n",
    "    remove_dir(os.path.join(MUSDB_PATH, 'test/'))\n",
    "    remove_dir(os.path.join(MUSDB_PATH, 'train/'))\n",
    "\n",
    "    extract_zip(os.path.join(MUSDB_PATH, 'musdb.zip'), MUSDB_PATH)\n",
    "\n",
    "    musdb.DB(root=MUSDB_PATH, download=False)\n",
    "else:\n",
    "    musdb.DB(root=MUSDB_PATH, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104162,
     "status": "ok",
     "timestamp": 1729764699616,
     "user": {
      "displayName": "Gabriel Meseguer Brocal",
      "userId": "00324772212969630818"
     },
     "user_tz": -120
    },
    "id": "eBb0uwG5xq3O",
    "outputId": "e66df90f-c2f4-45a6-bff8-afc2eebb2b70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer 9% filled.\n",
      "Buffer 19% filled.\n",
      "Buffer 29% filled.\n",
      "Buffer 39% filled.\n",
      "Buffer 49% filled.\n",
      "Buffer 59% filled.\n",
      "Buffer 69% filled.\n",
      "Buffer 79% filled.\n",
      "Buffer 89% filled.\n",
      "Buffer 99% filled.\n"
     ]
    }
   ],
   "source": [
    "class SourceSeparationDataloader:\n",
    "    \"\"\"\n",
    "    A custom data loader with shuffle buffer functionality.\n",
    "\n",
    "    Attributes:\n",
    "    dataset (torch.utils.data.Dataset): The dataset to shuffle.\n",
    "    buffer_size (int): Size of the shuffle buffer.\n",
    "        The bigger the buffer size is more audio segments  will be include.\n",
    "        Since we are getting all the segments of a song to speed up the process\n",
    "        we want this to be big to mix segments from different songs\n",
    "    batch_size (int): Size of each batch.\n",
    "    buffer (list): The buffer holding dataset elements.\n",
    "    dataset_iter (iterator): An iterator over the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: torch.utils.data.Dataset, buffer_size: int, batch_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a shuffle buffer for the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        dataset (torch.utils.data.Dataset): The dataset to shuffle.\n",
    "        buffer_size (int): Size of the shuffle buffer.\n",
    "        batch_size (int): Size of each batch.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = []\n",
    "        self.device = device\n",
    "        self.dataset_iter = iter(dataset)\n",
    "\n",
    "        for i in range(buffer_size):\n",
    "            if (i + 1) % (buffer_size // 10) == 0:\n",
    "                print(f\"Buffer {int(((i + 1) / buffer_size) * 100)}% filled.\")\n",
    "            self.buffer.append(next(self.dataset_iter))\n",
    "\n",
    "    def get_next(self) -> Tuple[torch.Tensor, torch.Tensor, List[str]]:\n",
    "        \"\"\"\n",
    "        Get the next batch of shuffled elements from the buffer.\n",
    "\n",
    "        Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor, List[str]]: A batch of randomly shuffled elements.\n",
    "        \"\"\"\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        batch_label = []\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            if len(self.buffer) == 0:\n",
    "                break  # If buffer is empty, stop forming the batch\n",
    "\n",
    "            idx = random.randint(0, len(self.buffer) - 1)\n",
    "            batch_x.append(self.buffer[idx][0])\n",
    "            batch_y.append(self.buffer[idx][1])\n",
    "            batch_label.append(self.buffer[idx][2])\n",
    "\n",
    "            try:\n",
    "                self.buffer[idx] = next(self.dataset_iter)\n",
    "            except StopIteration:\n",
    "                self.buffer.pop(idx)\n",
    "\n",
    "        return torch.stack(batch_x), torch.stack(batch_y), batch_label\n",
    "\n",
    "    def __iter__(self) -> 'SourceSeparationDataloader':\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> Tuple[torch.Tensor, torch.Tensor, List[str]]:\n",
    "        return self.get_next()\n",
    "\n",
    "\n",
    "class MusDBDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    A dataset for MusDB tracks with segment processing and multiprocessing\n",
    "    loading.\n",
    "\n",
    "    Attributes:\n",
    "    split (str): The dataset split ('train', 'valid', etc.).\n",
    "    targets (list): List of target audios ('vocals', 'drums', etc.).\n",
    "    segment_dur (float): Duration of each segment in seconds.\n",
    "    segment_overlap (float): Overlap between segments in seconds.\n",
    "    num_workers (int): Number of threads for parallel processing.\n",
    "    mus (musdb.DB): MusDB dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split: str,\n",
    "        targets: List[str],\n",
    "        segment_dur: float,\n",
    "        segment_overlap: float,\n",
    "        num_workers: int,\n",
    "        musdb_path: str = MUSDB_PATH,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the MusDBDataset.\n",
    "\n",
    "        Parameters:\n",
    "        split (str): The dataset split ('train', 'valid', etc.).\n",
    "        targets (list): List of target audios ('vocals', 'drums', etc.).\n",
    "        segment_dur (float): Duration of each segment in seconds.\n",
    "        segment_overlap (float): Overlap between segments in seconds.\n",
    "        num_workers (int): Number of threads for parallel processing.\n",
    "        \"\"\"\n",
    "\n",
    "        valid_targets = {'vocals', 'drums', 'bass', 'other'}\n",
    "        assert all(target in valid_targets for target in targets)\n",
    "\n",
    "        self.split = split\n",
    "        self.targets = targets\n",
    "        self.segment_dur = segment_dur\n",
    "        self.segment_overlap = segment_overlap\n",
    "        self.num_workers = num_workers\n",
    "        self.mus = musdb.DB(subsets=self.split, root=musdb_path)\n",
    "\n",
    "        if not self.mus.tracks:\n",
    "            raise ValueError(f\"The dataset for split '{self.split}' is empty or not loaded properly.\")\n",
    "\n",
    "    def _process_track(self, track) -> List[Tuple[torch.Tensor, torch.Tensor, str]]:\n",
    "        \"\"\"\n",
    "        Process a track by unfolding and rearranging its audio data.\n",
    "\n",
    "        Parameters:\n",
    "        track (musdb.Track): A musdb track object.\n",
    "\n",
    "        Returns:\n",
    "        list[tuple[torch.Tensor, torch.Tensor, str]]: Processed input and target audio segments.\n",
    "        \"\"\"\n",
    "        size = int(track.rate * self.segment_dur)\n",
    "        step = int(track.rate * self.segment_overlap)\n",
    "        name = track.name\n",
    "        rate = track.rate\n",
    "        x = torch.tensor(track.audio).to(torch.float32)\n",
    "        x = rearrange(x.unfold(dimension=0, size=size, step=step), 's c d -> s d c')\n",
    "        segments = []\n",
    "        for target in self.targets:\n",
    "            y = torch.tensor(track.targets[target].audio).to(torch.float32)\n",
    "            y = rearrange(y.unfold(dimension=0, size=size, step=step), 's c d -> s d c')\n",
    "            segments.extend(list(zip(x, y, [target]*x.shape[0])))\n",
    "        del(x, y, track)\n",
    "        return (segments, name, rate)\n",
    "\n",
    "\n",
    "    def __iter__(self) -> Iterator[Tuple[torch.Tensor, torch.Tensor, str]]:\n",
    "        \"\"\"\n",
    "        Iterate over the dataset with parallel track processing.\n",
    "\n",
    "        Yields:\n",
    "        tuple[torch.Tensor, torch.Tensor, str]: Segmented input and target audio.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.split == 'train':\n",
    "            # Continuous processing for training data\n",
    "            while True:\n",
    "                with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "                    tracks = [self.mus.tracks[i] for i in torch.randperm(len(self.mus.tracks)).tolist()][:self.num_workers]\n",
    "                    futures = [executor.submit(self._process_track, track) for track in tracks]\n",
    "                    for future in as_completed(futures):\n",
    "                        segments = future.result()[0]\n",
    "                        for x, y, l in segments:\n",
    "                            yield x, y, l\n",
    "        elif self.split == 'test':\n",
    "            # One-time processing for test data\n",
    "            with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "                futures = [executor.submit(self._process_track, track) for track in self.mus.tracks]\n",
    "                for future in as_completed(futures):\n",
    "                    yield future.result()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown split: {self.split}\")\n",
    "\n",
    "\n",
    "# Example of how to initialize the data loader\n",
    "duration = 4\n",
    "ds_train = SourceSeparationDataloader(MusDBDataset('train', ['vocals'], duration, duration/2, num_workers=8), buffer_size=2999, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1729764699616,
     "user": {
      "displayName": "Gabriel Meseguer Brocal",
      "userId": "00324772212969630818"
     },
     "user_tz": -120
    },
    "id": "h8V6xEqyN6i6",
    "outputId": "9a576cb2-511f-489f-c52b-821b1b14915c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 176400, 2]) torch.Size([32, 176400, 2]) ['vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals', 'vocals']\n"
     ]
    }
   ],
   "source": [
    "d = next(ds_train)\n",
    "print(d[0].shape, d[1].shape, d[2])\n",
    "del(ds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gA0xOQqGaEEB"
   },
   "source": [
    "## Spectral operations\n",
    "\n",
    "In this section, we cover the following spectral operations critical for the source separation model:\n",
    "\n",
    "* **Waveform Processing and STFT Computation**: Functions to process waveforms, compute the complex Short Fast Fourier Transform (STFT), and then use the inverse STFT (iSTFT) to revert to the waveform.\n",
    "\n",
    "* **Model Input and Output**: The input and output of the source separation model are waveforms.\n",
    "\n",
    "* **Internal Operations**: We compute the FFT of the input waveform. The deep neural network receives the real and imaginary parts of the FFT as two separate channels. The mask is applied independently to each channel (real/imaginary). The iFFT then converts the masked output back to the waveform.\n",
    "\n",
    "* **Model Blocks**: Detailed description of the model components responsible for these operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1729764699616,
     "user": {
      "displayName": "Gabriel Meseguer Brocal",
      "userId": "00324772212969630818"
     },
     "user_tz": -120
    },
    "id": "xsczlGB9aEEB"
   },
   "outputs": [],
   "source": [
    "def get_audio_prepro_args(\n",
    "    dur: float,\n",
    "    window: str = \"hanning\",\n",
    "    n_fft: int = 2048,\n",
    "    sr: int = 44100,\n",
    "    hop_factor: float = 0.5,\n",
    "    stereo: bool = True,\n",
    ") -> tuple[torch.Tensor, int, int, int, int, int, int, bool]:\n",
    "    \"\"\"\n",
    "    Prepare audio preprocessing arguments.\n",
    "\n",
    "    Parameters:\n",
    "    dur (float): Duration in seconds.\n",
    "    window (str): Window type. Default is \"hanning\".\n",
    "    n_fft (int): Number of FFT points. Default is 2048.\n",
    "    sr (int): Sample rate. Default is 44100.\n",
    "    hop_factor (float): Factor to calculate hop length. Default is 0.5.\n",
    "    stereo (bool): If True, stereo audio is used. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    tuple: window tensor, number of FFT points, hop length, sample rate, number of frames,\n",
    "           number of bins, length in samples, stereo flag.\n",
    "    \"\"\"\n",
    "    hop_fft = np.round(n_fft * hop_factor).astype(np.int32)\n",
    "    length_in_samples = int(np.ceil(dur * sr))\n",
    "    n_frames = int(np.ceil(length_in_samples / hop_fft))\n",
    "    n_bins = n_fft // 2 + 1\n",
    "    if window == \"hanning\":\n",
    "        w = torch.hann_window(n_fft)\n",
    "    return w, n_fft, hop_fft, sr, n_frames, n_bins, length_in_samples, stereo\n",
    "\n",
    "\n",
    "def view_as_real_img(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert complex tensor to a real image tensor.\n",
    "\n",
    "    Parameters:\n",
    "    x (torch.Tensor): Complex input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Real image tensor with separated real and imaginary parts.\n",
    "    \"\"\"\n",
    "    return torch.cat((x.real.unsqueeze(-1), x.imag.unsqueeze(-1)), dim=-1)\n",
    "\n",
    "\n",
    "def waveform2spec(\n",
    "    x: torch.Tensor,\n",
    "    window: torch.Tensor,\n",
    "    n_fft_audio: int,\n",
    "    fft_hop: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert waveform tensor to spectrogram tensor using Short-Time Fourier Transform (STFT).\n",
    "\n",
    "    Parameters:\n",
    "    x (torch.Tensor): Input waveform tensor.\n",
    "    window (torch.Tensor): Window function tensor.\n",
    "    n_fft_audio (int): Number of FFT points.\n",
    "    fft_hop (int): Hop length for STFT.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Spectrogram tensor with separated real and imaginary parts.\n",
    "    \"\"\"\n",
    "    return view_as_real_img(x.stft(n_fft=n_fft_audio, window=window, hop_length=fft_hop, return_complex=True).type(torch.complex64))\n",
    "\n",
    "\n",
    "class STFTModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Module for Short-Time Fourier Transform (STFT) on audio signals.\n",
    "\n",
    "    Attributes:\n",
    "    duration (float): Duration of audio signal.\n",
    "    window_fft (torch.Tensor): Window function tensor.\n",
    "    n_fft (int): Number of FFT points.\n",
    "    hop_fft (int): Hop length.\n",
    "    sr (int): Sample rate.\n",
    "    n_frames (int): Number of frames.\n",
    "    n_bins (int): Number of frequency bins.\n",
    "    length_in_samples (int): Length of audio signal in samples.\n",
    "    stereo (bool): If True, stereo audio is used.\n",
    "    stft_fn (function): Function for STFT computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, duration: float) -> None:\n",
    "        super(STFTModule, self).__init__()\n",
    "        self.window_fft, self.n_fft, self.hop_fft, self.sr, self.n_frames, self.n_bins, self.length_in_samples, self.stereo = get_audio_prepro_args(duration)\n",
    "        self.duration = duration\n",
    "        self.stft_fn = waveform2spec\n",
    "\n",
    "    def get_input_shape(self) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Get the input shape for the module.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Shape of input tensor.\n",
    "        \"\"\"\n",
    "        return self.length_in_samples, 2 if self.stereo else 1\n",
    "\n",
    "    def get_out_shape(self) -> list[int]:\n",
    "        \"\"\"\n",
    "        Get the output shape for the module.\n",
    "\n",
    "        Returns:\n",
    "        list: Shape of output tensor.\n",
    "        \"\"\"\n",
    "        return [4 if self.stereo else 2, int(self.n_frames), int(self.n_bins)]\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for the module.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Output tensor after STFT.\n",
    "        \"\"\"\n",
    "        shape = x.shape\n",
    "        x = rearrange(x, \"b s c -> (b c) s\")\n",
    "        x = self.stft_fn(x, self.window_fft.to(x.device), self.n_fft, self.hop_fft)\n",
    "        x = rearrange(x, \"(b c) f t r -> b (c r) t f\", b=shape[0], c=shape[-1])\n",
    "        return x\n",
    "\n",
    "\n",
    "class iSTFTModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Module for Inverse Short-Time Fourier Transform (iSTFT) on audio signals.\n",
    "\n",
    "    Attributes:\n",
    "    duration (float): Duration of audio signal.\n",
    "    window_fft (torch.Tensor): Window function tensor.\n",
    "    n_fft (int): Number of FFT points.\n",
    "    hop_fft (int): Hop length.\n",
    "    sr (int): Sample rate.\n",
    "    n_frames (int): Number of frames.\n",
    "    n_bins (int): Number of frequency bins.\n",
    "    length_in_samples (int): Length of audio signal in samples.\n",
    "    stereo (bool): If True, stereo audio is used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, duration: float) -> None:\n",
    "        super(iSTFTModule, self).__init__()\n",
    "        self.window_fft, self.n_fft, self.hop_fft, self.sr, self.n_frames, self.n_bins, self.length_in_samples, self.stereo = get_audio_prepro_args(duration)\n",
    "        self.duration = duration\n",
    "\n",
    "    def istft_fn(\n",
    "        self,\n",
    "        x_fft: torch.Tensor,\n",
    "        window: torch.Tensor,\n",
    "        n_fft_audio: int,\n",
    "        fft_hop: int,\n",
    "        length: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute Inverse Short-Time Fourier Transform (iSTFT).\n",
    "\n",
    "        Parameters:\n",
    "        x_fft (torch.Tensor): Input spectrogram tensor.\n",
    "        window (torch.Tensor): Window function tensor.\n",
    "        n_fft_audio (int): Number of FFT points.\n",
    "        fft_hop (int): Hop length for iSTFT.\n",
    "        length (int): Length of the output waveform.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Reconstructed waveform tensor.\n",
    "        \"\"\"\n",
    "        return x_fft.istft(\n",
    "            n_fft=n_fft_audio,\n",
    "            window=window,\n",
    "            hop_length=fft_hop,\n",
    "            return_complex=False,\n",
    "            length=length,\n",
    "        )\n",
    "\n",
    "    def get_input_shape(self) -> list[int]:\n",
    "        \"\"\"\n",
    "        Get the input shape for the module.\n",
    "\n",
    "        Returns:\n",
    "        list: Shape of input tensor.\n",
    "        \"\"\"\n",
    "        return [4 if self.stereo else 2, self.n_frames, self.n_bins]\n",
    "\n",
    "    def get_out_shape(self) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Get the output shape for the module.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Shape of output tensor.\n",
    "        \"\"\"\n",
    "        return self.length_in_samples, 2 if self.stereo else 1\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the module.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor after iSTFT.\n",
    "        \"\"\"\n",
    "        shape = x.shape\n",
    "        x = rearrange(x, \"b (c r) t f -> (b c) f t r\", r=2, c=shape[1] // 2)\n",
    "        x = torch.view_as_complex(x.contiguous())\n",
    "        x = self.istft_fn(x, self.window_fft.to(x.device), self.n_fft, self.hop_fft, self.length_in_samples)\n",
    "        return rearrange(x, \"(b c) s -> b s c\", b=shape[0], c=shape[1] // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oa5yXCPYaEEB"
   },
   "source": [
    "## Blocks\n",
    "\n",
    "Basic building blocks for our Source Separation UNet:\n",
    "\n",
    "*   **Convolutional block for the Encoder**: This block is essential for capturing spatial hierarchies and feature representations from the input data.\n",
    "*   **Up-convolution for the Decoder**: This includes an interpolation function for up-sampling any tensor dimension to match the corresponding skip connection dimension.\n",
    "\n",
    "These foundational blocks can be seamlessly swapped out for more advanced options, such as ConvNeXt, to enhance model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1729764699859,
     "user": {
      "displayName": "Gabriel Meseguer Brocal",
      "userId": "00324772212969630818"
     },
     "user_tz": -120
    },
    "id": "6iQO357vaEEB"
   },
   "outputs": [],
   "source": [
    "def interpolate_tensor(x: torch.Tensor, shape: tuple[int, int], mode: str = \"bilinear\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Interpolate a tensor to a given shape using specified interpolation mode.\n",
    "\n",
    "    Parameters:\n",
    "    x (torch.Tensor): Input tensor.\n",
    "    shape (tuple[int, int]): Target shape for interpolation.\n",
    "    mode (str): Interpolation mode. Default is \"bilinear\".\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Interpolated tensor.\n",
    "    \"\"\"\n",
    "    return F.interpolate(x, size=shape, mode=mode, align_corners=False, antialias=False)\n",
    "\n",
    "\n",
    "def get_activation(name: str) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Retrieve activation function by name.\n",
    "\n",
    "    Parameters:\n",
    "    name (str): Name of the activation function.\n",
    "\n",
    "    Returns:\n",
    "    nn.Module: Activation function module.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the activation function name is not found.\n",
    "    \"\"\"\n",
    "    activation_map = {\n",
    "        \"relu\": nn.ReLU(),\n",
    "        \"leaky_relu\": nn.LeakyReLU(),\n",
    "        \"gelu\": nn.GELU(),\n",
    "        \"prelu\": torch.nn.PReLU(),\n",
    "        \"tanh\": nn.Tanh(),\n",
    "        \"identity\": nn.Identity(),\n",
    "    }\n",
    "    if name in activation_map:\n",
    "        return activation_map[name]\n",
    "    else:\n",
    "        raise ValueError(f\"Activation function {name} not found.\")\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional block with optional dropout and activation.\n",
    "\n",
    "    Attributes:\n",
    "    in_channels (int): Number of input channels.\n",
    "    out_channels (int): Number of output channels.\n",
    "    ops (nn.ModuleList): List of operations in the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[tuple[int, int], int] = 3,\n",
    "        dropout: bool = False,\n",
    "        activation: str = 'relu',\n",
    "        **kargs: dict\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ConvBlock.\n",
    "\n",
    "        Parameters:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        kernel_size (Union[tuple[int, int], int]): Size of the convolving kernel. Default is 3.\n",
    "        dropout (bool): If True, includes a dropout layer. Default is True.\n",
    "        activation (str): Activation function name. Default is 'relu'.\n",
    "        **kargs (dict): Additional keyword arguments for Conv2d.\n",
    "        \"\"\"\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.ops = nn.ModuleList()\n",
    "\n",
    "        self.ops.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                **kargs\n",
    "            )\n",
    "        )\n",
    "        self.ops.append(nn.BatchNorm2d(out_channels))\n",
    "        if dropout:\n",
    "            self.ops.append(nn.Dropout(p=0.25))\n",
    "        self.ops.append(get_activation(activation))\n",
    "\n",
    "    def get_out_shape(self, shape: tuple[int, int]) -> list[int]:\n",
    "        \"\"\"\n",
    "        Compute the output shape of the block given input shape.\n",
    "\n",
    "        Parameters:\n",
    "        shape (tuple[int, int]): Input shape.\n",
    "\n",
    "        Returns:\n",
    "        list[int]: Output shape.\n",
    "        \"\"\"\n",
    "        def to_dim_fn(v: int, p: int, d: int, k: int, s: int) -> int:\n",
    "            return int(1 + ((v + 2 * p - d * (k - 1) - 1) / s))\n",
    "\n",
    "        if self.ops[0].padding != \"same\":\n",
    "            p = self.ops[0].padding\n",
    "            d = self.ops[0].dilation\n",
    "            k = self.ops[0].kernel_size\n",
    "            s = self.ops[0].stride\n",
    "            a = to_dim_fn(shape[1], p[0], d[0], k[0], s[0])\n",
    "            b = to_dim_fn(shape[2], p[1], d[1], k[1], s[1])\n",
    "        else:\n",
    "            a, b = shape[1:]\n",
    "        return [self.out_channels, a, b]\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the convolutional block.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        for op in self.ops:\n",
    "            x = op(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpConvBlock(ConvBlock):\n",
    "    \"\"\"\n",
    "    Upsampling convolutional block with interpolation.\n",
    "\n",
    "    Attributes:\n",
    "    out_shape (list[int]): Target shape after upsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, out_shape: list[int], **kargs: dict) -> None:\n",
    "        super(UpConvBlock, self).__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            **kargs,\n",
    "        )\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "    def get_out_shape(self) -> list[int]:\n",
    "        \"\"\"\n",
    "        Get the output shape for the block.\n",
    "\n",
    "        Returns:\n",
    "        list[int]: Output shape.\n",
    "        \"\"\"\n",
    "        return [self.out_channels, *self.out_shape]\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the upsampling block.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        x = interpolate_tensor(x, self.out_shape[1:])\n",
    "        return super(UpConvBlock, self).forward(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1eAia6WaEEC"
   },
   "source": [
    "## Model\n",
    "\n",
    "Code for creating the model:\n",
    "\n",
    "* **STFT**: Computes the spectral transformation from the waveform.\n",
    "\n",
    "* **Encoder**: Codifies and highlights the relevant information from the signal. It downsamples the input tensor (the complex STFT with imaginary and real components stored independently per channel) by a factor of 2 in both time and frequency dimensions, while simultaneously increasing the number of channels by a power of 2.\n",
    "\n",
    "* **Decoder**: Transforms the latent space back to the original input. It has skip connections to retrieve fine-grain details in the signal. The output maintains the same dimensions as the input to the encoder, allowing it to be used either as a mask for the input or as a final output signal. Both cases will then be fed into the iSTFT.\n",
    "\n",
    "* **iSTFT**: Computes the waveform from the complex spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1729764699859,
     "user": {
      "displayName": "Gabriel Meseguer Brocal",
      "userId": "00324772212969630818"
     },
     "user_tz": -120
    },
    "id": "wPX6l4TmaEEC"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module consisting of multiple convolutional blocks.\n",
    "\n",
    "    Attributes:\n",
    "    n_layers (int): Number of layers in the encoder.\n",
    "    ops (nn.ModuleList): List of convolutional operations.\n",
    "    block_shapes (List[List[int]]): List of shapes of the blocks in the encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_input_shape: List[int],\n",
    "        n_layers: int = 6,\n",
    "        n_filters: int = 16,\n",
    "        max_n_filters: int = 512\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Encoder module.\n",
    "\n",
    "        Parameters:\n",
    "        block_input_shape (List[int]): Shape of the input block.\n",
    "        n_layers (int): Number of layers in the encoder. Default is 6.\n",
    "        n_filters (int): Initial number of filters for the convolutions. Default is 16.\n",
    "        max_n_filters (int): Maximum number of filters for the convolutions. Default is 512.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.ops = nn.ModuleList([])\n",
    "        self.block_shapes = [block_input_shape]\n",
    "        kargs = {\n",
    "            \"out_channels\": n_filters,\n",
    "            \"stride\": 2,\n",
    "            \"in_channels\": block_input_shape[0]\n",
    "        }\n",
    "        for _ in range(self.n_layers):\n",
    "            self.ops.append(ConvBlock(**kargs))\n",
    "            block_input_shape = self.ops[-1].get_out_shape(block_input_shape)\n",
    "            self.block_shapes.append(block_input_shape)\n",
    "            kargs['in_channels'] = self.block_shapes[-1][0]\n",
    "            kargs['out_channels'] = min(self.block_shapes[-1][0] * 2, max_n_filters)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> list[Union[torch.Tensor, Any]]:\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        list[Union[torch.Tensor, Any]]: List of outputs from each layer of the encoder.\n",
    "        \"\"\"\n",
    "        outputs = [x]\n",
    "        for op in self.ops:\n",
    "            x = op(x)\n",
    "            outputs.append(x)\n",
    "        return outputs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module for upsampling and reconstructing the input.\n",
    "\n",
    "    Attributes:\n",
    "    mask_act (str): Activation function for the mask.\n",
    "    encoder_block_shapes (List[List[Any]]): Shapes of the encoder blocks.\n",
    "    ops (nn.ModuleList): List of upsampling convolutional operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_block_shapes: List[List[Any]],\n",
    "        mask_act: str,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Decoder module.\n",
    "\n",
    "        Parameters:\n",
    "        encoder_block_shapes (List[List[Any]]): Shapes of the encoder blocks.\n",
    "        mask_act (str): Activation function for the mask.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.mask_act = mask_act\n",
    "        self.encoder_block_shapes = encoder_block_shapes\n",
    "        self.ops = nn.ModuleList([])\n",
    "\n",
    "        for i in range(len(self.encoder_block_shapes) - 1):\n",
    "            in_channels = self.encoder_block_shapes[i][0] * 2 if i != 0 else self.encoder_block_shapes[i][0]\n",
    "            kargs = {\n",
    "                'out_shape': self.encoder_block_shapes[i+1],\n",
    "                'in_channels': in_channels,\n",
    "                'out_channels': self.encoder_block_shapes[i+1][0],\n",
    "                'padding': 'same',\n",
    "            }\n",
    "            if i == len(self.encoder_block_shapes) - 2:\n",
    "                kargs['activation'] = mask_act\n",
    "            self.ops.append(UpConvBlock(**kargs))\n",
    "\n",
    "    def forward(self, *x: Any) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder.\n",
    "\n",
    "        Parameters:\n",
    "        x (Any): Input tensors.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor after upsampling and reconstruction.\n",
    "        \"\"\"\n",
    "        x, encoder_outputs = x[0], x[1:]\n",
    "        for i in range(len(encoder_outputs)):\n",
    "            if i < len(encoder_outputs) - 1:\n",
    "                x = torch.cat((self.ops[i](x), encoder_outputs[i]), dim=1)\n",
    "            else:\n",
    "                x = self.ops[i](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SourceSeparation(nn.Module):\n",
    "    \"\"\"\n",
    "    Source separation module using STFT, Encoder, Decoder, and iSTFT.\n",
    "\n",
    "    Attributes:\n",
    "    duration (float): Duration of the audio.\n",
    "    behavior (str): Behavior for the output (\"masking\" or \"mapping\").\n",
    "    stft (STFTModule): Short-Time Fourier Transform module.\n",
    "    encoder (Encoder): Encoder module.\n",
    "    decoder (Decoder): Decoder module.\n",
    "    istft (iSTFTModule): Inverse Short-Time Fourier Transform module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        duration: float,\n",
    "        mask_act: str = \"tanh\",\n",
    "        behavior: str = \"masking\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the SourceSeparation module.\n",
    "\n",
    "        Parameters:\n",
    "        duration (float): Duration of the audio.\n",
    "        mask_act (str): Activation function for the mask. Default is \"tanh\".\n",
    "        behavior (str): Behavior for the output. Either \"masking\" or \"mapping\". Default is \"masking\".\n",
    "        \"\"\"\n",
    "        super(SourceSeparation, self).__init__()\n",
    "        assert behavior in [\"masking\", \"mapping\"]\n",
    "        self.duration = duration\n",
    "        self.behavior = behavior\n",
    "        self.stft = STFTModule(duration)\n",
    "        self.encoder = Encoder(self.stft.get_out_shape())\n",
    "        self.decoder = Decoder(\n",
    "            encoder_block_shapes=self.encoder.block_shapes[::-1],\n",
    "            mask_act=mask_act\n",
    "        )\n",
    "        self.istft = iSTFTModule(duration)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the source separation module.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor after source separation.\n",
    "        \"\"\"\n",
    "\n",
    "        # we transform the waveform into a time-frequency representation\n",
    "        x = self.stft(x)\n",
    "\n",
    "        \"\"\"\n",
    "        The encoder takes spec with the real/img info on each channel and processes\n",
    "        it through multiple convolutional layers. This step essentially extracts features\n",
    "        and compresses the audio representation.\n",
    "        \"\"\"\n",
    "        x_encoder = self.encoder(x)\n",
    "\n",
    "        \"\"\"\n",
    "        The decoder takes the output from the encoder and reconstructs the separated\n",
    "        audio signal. It uses upsampling and skip connections to recover fine details.\n",
    "        \"\"\"\n",
    "        y = self.decoder(*x_encoder[::-1])\n",
    "\n",
    "        if self.behavior == \"masking\":\n",
    "            y = torch.multiply(x, y)\n",
    "\n",
    "        return self.istft(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1ssuk5ij5b4"
   },
   "source": [
    "### Loss\n",
    "\n",
    "Several waveform loss approaches exist. We use only the basic cosine distance loss, which optimizes well and efficiently guides the model, though not perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1729764699859,
     "user": {
      "displayName": "Gabriel Meseguer Brocal",
      "userId": "00324772212969630818"
     },
     "user_tz": -120
    },
    "id": "ou85cLX6shdk"
   },
   "outputs": [],
   "source": [
    "def cosine_loss() -> torch.nn.CosineSimilarity:\n",
    "    \"\"\"\n",
    "    Create a cosine similarity-based loss function.\n",
    "\n",
    "    Returns:\n",
    "    torch.nn.CosineSimilarity: Cosine similarity function with an embedded loss function.\n",
    "    \"\"\"\n",
    "    cosine = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
    "    \"\"\"\n",
    "    (1 - cosine) to have a loss between 2 and 0.\n",
    "    Minimizing it will do the signals be close together\n",
    "    \"\"\"\n",
    "    loss_fn = lambda y_pred, y_true: 1 - cosine(y_pred, y_true)\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzpVf7Ikjt5y"
   },
   "source": [
    "## Training\n",
    "\n",
    "We create the data loader, the model, the optimizer, and the loss function. The data loader always provides a batch, and each batch should differ from the previous ones (especially with data augmentation, which is common in source separation but not in our case). In this context, the idea of an epoch is not well-defined. Instead, we use an epoch as a moment to validate the model and save the training progress (not implemented in this tutorial), but it doesn't necessarily mean completing one iteration over the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "executionInfo": {
     "elapsed": 159978,
     "status": "error",
     "timestamp": 1729764859834,
     "user": {
      "displayName": "Gabriel Meseguer Brocal",
      "userId": "00324772212969630818"
     },
     "user_tz": -120
    },
    "id": "hX9vIlI6jt5y",
    "outputId": "6b93be09-cff4-4359-91e4-c4c86d5fb500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer 9% filled.\n",
      "Buffer 19% filled.\n",
      "Buffer 29% filled.\n",
      "Buffer 39% filled.\n",
      "Buffer 49% filled.\n",
      "Buffer 59% filled.\n",
      "Buffer 69% filled.\n",
      "Buffer 79% filled.\n",
      "Buffer 89% filled.\n",
      "Buffer 99% filled.\n",
      "\n",
      " epoch 0\n",
      "\u001b[1m  2/513\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:11:58\u001b[0m 30s/step - train_loss: 0.7753"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-69b0e0ba694d>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSourceSeparationDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMusDBDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'vocals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSourceSeparation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-69b0e0ba694d>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, ds_iter, device, n_epochs, n_steps, loss_fn, lr)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mmix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-70d150756ad8>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(y_pred, y_true)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mMinimizing\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mdo\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msignals\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mclose\u001b[0m \u001b[0mtogether\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/distance.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def training_loop(\n",
    "    model,\n",
    "    ds_iter: Any,\n",
    "    device: str,\n",
    "    n_epochs: int = 50,\n",
    "    n_steps: int = 512,\n",
    "    loss_fn: torch.nn.CosineSimilarity  = cosine_loss(),\n",
    "    lr: float = 5e-4,\n",
    ") -> Tuple[float, bool]:\n",
    "    \"\"\"\n",
    "    Run the training loop for the given model.\n",
    "\n",
    "    Parameters:\n",
    "    model: The model to be trained.\n",
    "    ds_iter (Any): An iterator over the dataset.\n",
    "    device (str): The device to use for training (e.g., 'cuda' or 'cpu').\n",
    "    n_epochs (int): Number of epochs to train. Default is 10.\n",
    "    n_steps (int): Number of steps per epoch. Default is 512.\n",
    "    loss_fn (torch.nn.CosineSimilarity): The loss function to use. Default is cosine_loss().\n",
    "    lr (float): Learning rate for the optimizer. Default is 5e-4.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, bool]: The trained model and a boolean indicating success.\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    # --- TRAINING ---\n",
    "    for i in range(n_epochs):\n",
    "        print('\\n epoch {}'.format(i))\n",
    "        model.train()\n",
    "        progress_bar = Progbar(n_steps + 1)\n",
    "        for j in range(n_steps):\n",
    "            batch = next(ds_iter)\n",
    "            optimizer.zero_grad()\n",
    "            mix = batch[0].to(device)\n",
    "            target = batch[1].to(device)\n",
    "            loss = loss_fn(model(mix), target).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            progress_bar.update(j, [(\"train_loss\", loss.item())])\n",
    "            torch.cuda.empty_cache()\n",
    "    return model\n",
    "\n",
    "\n",
    "duration = 4\n",
    "# Initialize the data loader\n",
    "ds_train = SourceSeparationDataloader(MusDBDataset('train', ['vocals'], duration, duration/2, num_workers=4), 1999, 32)\n",
    "model = SourceSeparation(duration=duration).to(device)\n",
    "model = training_loop(model, ds_train, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxSyX-qIkBr3"
   },
   "source": [
    "## Evaluation\n",
    "In this section, you will:\n",
    "\n",
    "* Loop over the **MusDBDataset**: Ensure that all stems for a given song is provided. No shuffle is needed since we can evaluate each track independly. We go over the test set once and then finish the computation.\n",
    "\n",
    "* Compute Predictions with the Model: Generate predictions for each segment using the model.\n",
    "\n",
    "* Prepare Stems for Metrics Calculation: Always include both the source (real and predicted) and the \"residual\" (mixture - source) to compute the metrics accurately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MsHwdbqDkBr3",
    "outputId": "639e90f0-85a8-4e4a-f527-e8277418ab98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing track Angels In Amplifiers - I'm Alright\n"
     ]
    }
   ],
   "source": [
    "def get_metrics(\n",
    "    ref: npt.NDArray[np.float32],\n",
    "    est: npt.NDArray[np.float32],\n",
    "    sr: int,\n",
    "    window: int = 1,\n",
    "    hop: int = 1,\n",
    ") -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Computes and logs various audio separation metrics.\n",
    "\n",
    "    Parameters:\n",
    "    ref (npt.NDArray[np.float32]): Reference signal.\n",
    "    est (npt.NDArray[np.float32]): Estimated signal.\n",
    "    sr (int): Sampling rate.\n",
    "    window (int): Window size in seconds. Defaults to 1.\n",
    "    hop (int): Hop size in seconds. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Dict[str, int]]: A dictionary containing metrics for vocals and accompaniment (acc).\n",
    "    \"\"\"\n",
    "    print(\"Computing the metrics\")\n",
    "    sdr, isr, sir, sar, _ = bss_eval(\n",
    "        ref,\n",
    "        est,\n",
    "        window=window * sr,\n",
    "        hop=hop * sr,\n",
    "        framewise_filters=False,\n",
    "        bsseval_sources_version=False,\n",
    "        compute_permutation=False,\n",
    "    )\n",
    "    print(\"Done!\")\n",
    "    output[\"target\"] = {\"sdr\": sdr[0], \"isr\": isr[0], \"sir\": sir[0], \"sar\": sar[0]}\n",
    "    output[\"rest\"] = {\"sdr\": sdr[1], \"isr\": isr[1], \"sir\": sir[1], \"sar\": sar[1]}\n",
    "\n",
    "    # The median is the most standard way of comparing metrics on source separation\n",
    "    print(\n",
    "        \"Results target: \\n\\tSDR: {}, \\n\\tSIR: {}, \\n\\tSAR: {}\".format(\n",
    "            np.nanmedian(sdr[0]), np.nanmedian(sir[0]), np.nanmedian(sar[0])\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Results rest: \\n\\tSDR: {}, \\n\\tSIR: {}, \\n\\tSAR: {}\".format(\n",
    "            np.nanmedian(sdr[1]), np.nanmedian(sir[1]), np.nanmedian(sar[1])\n",
    "        )\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "ds_test = MusDBDataset('test', ['vocals'], duration, duration, num_workers=8)\n",
    "\n",
    "model.eval()\n",
    "output = {}\n",
    "for track, name, rate in iter(ds_test):\n",
    "    print(\"Processing track {}\".format(name))\n",
    "    m = torch.stack(list(zip(*track))[0], dim=0)\n",
    "    with torch.no_grad():\n",
    "        p = model(m.to(device)).detach().cpu()\n",
    "\n",
    "    m = rearrange(m, 'b s c -> (b s) c')\n",
    "    p = rearrange(p, 'b s c -> (b s) c')\n",
    "    t = torch.cat(list(zip(*track))[1], dim=0)\n",
    "\n",
    "    ref = torch.stack((t, m - t), dim=0)\n",
    "    est = torch.stack((p, m - p), dim=0)\n",
    "    output[name] = get_metrics(ref, est, rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_17GOE_B88P"
   },
   "source": [
    "## Conditioning\n",
    "\n",
    "In this section, we delve into the use of FiLM layers for conditioning. These layers offer a straightforward method to tailor audio features based on the target instrument. While our implementation focuses on applying FiLM layers at the bottleneck layer of the encoder, this approach can be easily adapted to other layers, providing flexibility in your audio processing pipeline. This versatility ensures that you can fine-tune the conditioning to best suit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQJ44D0CCAFr"
   },
   "outputs": [],
   "source": [
    "class FiLMBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature-wise Linear Modulation (FiLM) block for conditioning.\n",
    "\n",
    "    Attributes:\n",
    "    gammas (nn.Embedding): Embedding layer for gamma values.\n",
    "    betas (nn.Embedding): Embedding layer for beta values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels_bottleneck: int, valid_targets: List[str] = ['vocals', 'drums', 'bass', 'other'],) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the FiLMBlock.\n",
    "        This block learns instrument-specific scaling and shifting factors,\n",
    "        which are applied to the features before decoding.\n",
    "\n",
    "        Parameters:\n",
    "        n_channels_bottleneck (int): Number of bottleneck channels.\n",
    "        valid_targets (List[str]): List of valid targets. Default is ['vocals', 'drums', 'bass', 'other'].\n",
    "\n",
    "        \"\"\"\n",
    "        super(FiLMBlock, self).__init__()\n",
    "        self.valid_targets = valid_targets\n",
    "        \"\"\"\n",
    "        Embedding layers are a way to represent categorical data\n",
    "        (like instrument names) as numerical vectors that the network can\n",
    "        understand. gammas and betas are learned parameters that will be\n",
    "        used to modulate the audio features.\n",
    "        \"\"\"\n",
    "        self.gammas = nn.Embedding(len(valid_targets), n_channels_bottleneck)\n",
    "        self.betas = nn.Embedding(len(valid_targets), n_channels_bottleneck)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, ctxt: List) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the FiLM block.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        ctxt (torch.Tensor): Context tensor with indices of the desired instrument.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Modulated tensor.\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        # Instrument names to numerical indices using the valid_targets list.\n",
    "        ctxt = torch.tensor([self.valid_targets.index(item) for item in ctxt]).to(device)\n",
    "        # We retrieve the right gamma and beta values from the embedding layers.\n",
    "        gammas = rearrange(self.gammas(ctxt), 'b c -> b c 1 1')\n",
    "        betas = rearrange(self.betas(ctxt), 'b c -> b c 1 1')\n",
    "        # It returns the modulated audio features.\n",
    "        x = gammas * x + betas\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConditoningSourceSeparation(SourceSeparation):\n",
    "    \"\"\"\n",
    "    Source separation model with conditioning using FiLM.\n",
    "\n",
    "    Attributes:\n",
    "    conditioning (FiLMBlock): FiLM block for conditioning.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        duration: float,\n",
    "        **kargs: dict\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ConditoningSourceSeparation module.\n",
    "\n",
    "        Parameters:\n",
    "        duration (float): Duration of the audio.\n",
    "        **kargs (dict): Additional keyword arguments for SourceSeparation.\n",
    "        \"\"\"\n",
    "        super(ConditoningSourceSeparation, self).__init__(duration=duration, **kargs)\n",
    "        self.conditioning = FiLMBlock(n_channels_bottleneck=self.encoder.block_shapes[-1][0])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, ctxt: List) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the conditioning source separation model.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        ctxt (torch.Tensor): Context tensor with indices of the desired instrument.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor after source separation and conditioning.\n",
    "        \"\"\"\n",
    "        x = self.stft(x)\n",
    "        x_encoder = self.encoder(x)\n",
    "        # Conditioning happens only at the bottleneck of the encoder.\n",
    "        x_encoder[-1] = self.conditioning(x_encoder[-1], ctxt)\n",
    "        y = self.decoder(*x_encoder[::-1])\n",
    "        if self.behavior == \"masking\":\n",
    "            y = torch.multiply(x, y)\n",
    "        return self.istft(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nKmSBttqAc4"
   },
   "source": [
    "We updated training loop, which closely mirrors the previous one. The primary distinction is that the model now receives two parameters: the mixture and the desired instrument. Additionally, the dataloader has been enhanced to provide target audio segments from various instruments and different songs, ensuring a richer and more diverse training experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1729764859835,
     "user": {
      "displayName": "Gabriel Meseguer Brocal",
      "userId": "00324772212969630818"
     },
     "user_tz": -120
    },
    "id": "jj1X4gISOxhI"
   },
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    model,\n",
    "    ds_iter: Any,\n",
    "    device: str,\n",
    "    n_epochs: int = 50,\n",
    "    n_steps: int = 512,\n",
    "    loss_fn: torch.nn.CosineSimilarity  = cosine_loss(),\n",
    "    lr: float = 5e-4,\n",
    ") -> Tuple[float, bool]:\n",
    "\n",
    "    \"\"\"\n",
    "    Run the training loop for the given model.\n",
    "\n",
    "    Parameters:\n",
    "    model: The model to be trained.\n",
    "    ds_iter (Any): An iterator over the dataset.\n",
    "    device (str): The device to use for training (e.g., 'cuda' or 'cpu').\n",
    "    n_epochs (int): Number of epochs to train. Default is 10.\n",
    "    n_steps (int): Number of steps per epoch. Default is 512.\n",
    "    loss_fn (torch.nn.CosineSimilarity): The loss function to use. Default is cosine_loss().\n",
    "    lr (float): Learning rate for the optimizer. Default is 5e-4.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, bool]: The trained model and a boolean indicating success.\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    # --- TRAINING ---\n",
    "    for i in range(n_epochs):\n",
    "        print('\\n epoch {}'.format(i))\n",
    "        model.train()\n",
    "        progress_bar = Progbar(n_steps + 1)\n",
    "        for j in range(n_steps):\n",
    "            batch = next(ds_iter)\n",
    "            optimizer.zero_grad()\n",
    "            mix = batch[0].to(device)\n",
    "            target = batch[1].to(device)\n",
    "            label = batch[2]\n",
    "            loss = loss_fn(model(mix, label), target).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            progress_bar.update(j, [(\"train_loss\", loss.item())])\n",
    "            torch.cuda.empty_cache()\n",
    "    return model\n",
    "\n",
    "\n",
    "duration = 4\n",
    "instruments = ['vocals', 'drums', 'bass', 'other']\n",
    "# Initialize the data loader\n",
    "ds_train = SourceSeparationDataloader(MusDBDataset('train', instruments, duration, duration/2, num_workers=8), 3999, 32)\n",
    "cmodel = ConditoningSourceSeparation(duration=duration).to(device)\n",
    "cmodel = training_loop(cmodel, ds_train, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1729764859835,
     "user": {
      "displayName": "Gabriel Meseguer Brocal",
      "userId": "00324772212969630818"
     },
     "user_tz": -120
    },
    "id": "4Ybg6-156q1L"
   },
   "outputs": [],
   "source": [
    "ds_test = MusDBDataset('test', instruments, duration, duration, num_workers=8)\n",
    "model.eval()\n",
    "output = {}\n",
    "\n",
    "for track, name, rate in iter(ds_test):\n",
    "    print(\"Processing track {}\".format(name))\n",
    "    ll = list(zip(*track))[2]\n",
    "    mm = torch.stack(list(zip(*track))[0], dim=0)\n",
    "    tt = torch.cat(list(zip(*track))[1], dim=0)\n",
    "    for i, (instrument, ctxt) in enumerate(itertools.groupby(ll)):\n",
    "        print(\"Separating {}\".format(instrument))\n",
    "        ctxt = list(ctxt)\n",
    "        tmp = len(ctxt)\n",
    "        m = mm[i:i+tmp]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            p = cmodel(m.to(device), ctxt).detach().cpu()\n",
    "\n",
    "        m = rearrange(m, 'b s c -> (b s) c')\n",
    "        p = rearrange(p, 'b s c -> (b s) c')\n",
    "        t = tt[:m.shape[0]]\n",
    "\n",
    "        ref = torch.stack((t, m - t), dim=0)\n",
    "        est = torch.stack((p, m - p), dim=0)\n",
    "\n",
    "        output.setdefault(instrument, {})\n",
    "        output[instrument][name] = get_metrics(ref, est, rate)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
