{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0YSDoX64cXk"
      },
      "source": [
        "# Tutorial Auto-Tagging using different front-end/ different architecture\n",
        "\n",
        "- date: 2024-10-22\n",
        "- author: geoffroy.peeters@telecom-paris.fr\n",
        "\n",
        "This notebook was created for the tutorial held at ISMIR-2024 \"Deep-Learning 101 for Audio-based Music Information Retrieval\".\n",
        "If you use this notebook, please cite\n",
        "```\n",
        "@book{deeplearning-101-audiomir:book,\n",
        "\tauthor = {Peeters, Geoffroy and Meseguer-Brocal, Gabriel and Riou, Alain and Lattner, Stefan},\n",
        "\ttitle = {Deep Learning 101 for Audio-based MIR, ISMIR 2024 Tutorial},\n",
        "\turl = {https://geoffroypeeters.github.io/deeplearning-101-audiomir_book},\n",
        "\taddress = {San Francisco, USA},\n",
        "\tyear = {2024},\n",
        "\tmonth = November,\n",
        "\tdoi = {???},\n",
        "}\n",
        "```\n",
        "\n",
        "It illustrates the use of various deep-learning bricks to solve the task of \"Auto-Tagging\".\n",
        "\n",
        "Part of the code is based on\n",
        "- SincNet model: https://github.com/mravanelli/SincNet/blob/master/dnn_models.py\n",
        "\n",
        "Datasets are available at\n",
        "- GTZAN-Genre\n",
        "- MTT (Magna-Tag-a-Tune) https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset\n",
        "- RWC AIST https://staff.aist.go.jp/m.goto/RWC-MDB/AIST-Annotation/\n",
        "\n",
        "The two datasets illustrate different type of problems.\n",
        "- GTZAN-Genre is a multi-class problem (classification into 10 mutually exclusive classes)\n",
        "- MTT (Magna-Tag-a-Tune) is a multi-label (classification into 10 non-mutually exclusive tags)\n",
        "- RWC AIST is a chord-annotation datasets (multi-class over time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ4jXEM74cXl"
      },
      "source": [
        "## Deployment\n",
        "\n",
        "In case the notebook is run on GoogleColab or others, we first need to\n",
        "- get the packages: `git clone``\n",
        "- get the datasets\n",
        "\n",
        "We also test the models on two different datasets:\n",
        "- GTZAN-Genre\n",
        "- MTT (Magna-Tag-A-Tune)\n",
        "- RWC-Popular"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "%rm -rf /content/deeplearning-101-audiomir_notebook\n",
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8g-YH62s4iNk",
        "outputId": "e5123578-8043-4cd6-8619-31567605aaef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VW-a0HG54cXm",
        "outputId": "032d2d60-0106-48b5-8dae-3c07e26baa20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deeplearning-101-audiomir_notebook'...\n",
            "remote: Enumerating objects: 245, done.\u001b[K\n",
            "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 245 (delta 53), reused 56 (delta 26), pack-reused 146 (from 1)\u001b[K\n",
            "Receiving objects: 100% (245/245), 63.75 MiB | 12.03 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n",
            "/content/deeplearning-101-audiomir_notebook\n",
            "config_autotagging.yaml\n",
            "config_bittner.yaml\n",
            "config_chord.yaml\n",
            "config_cover.yaml\n",
            "config_doras.yaml\n",
            "feature.py\n",
            "model_factory.py\n",
            "model_module.py\n",
            "README.md\n",
            "TUTO_task_Auto_Tagging.ipynb\n",
            "TUTO_task_Auto_Tagging.ipynb_D1-I1-C1.ipynb\n",
            "TUTO_task_Auto_Tagging.ipynb_D1-I2-C2.ipynb\n",
            "TUTO_task_Auto_Tagging.ipynb_D1-I2-C3.ipynb\n",
            "TUTO_task_Auto_Tagging.ipynb_D1-I2-C4.ipynb\n",
            "TUTO_task_Auto_Tagging.ipynb_D2-I1-C1.ipynb\n",
            "TUTO_task_Auto_Tagging.ipynb_D3-I3-Chord.ipynb\n",
            "TUTO_task_Cover_Song_Identification.ipynb\n",
            "TUTO_task_Cover_Song_Identification.ipynb_cover1000.ipynb\n",
            "TUTO_task_Cover_Song_Identification.ipynb_datacos.ipynb\n",
            "TUTO_task_Generation_Autoregressive.ipynb\n",
            "TUTO_task_Generation_Diffusion.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D1-I1-C1.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D1-I2-C1.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D1-I2-C2.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D1-I2-C3.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D1-I2-C4.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D1-I2-Unet.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D2-I2-C1.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D2-I2-C3.ipynb\n",
            "TUTO_task_Multi_Pitch_Estimation.ipynb_D2-I2-Unet.ipynb\n"
          ]
        }
      ],
      "source": [
        "do_deploy = True\n",
        "\n",
        "if do_deploy:\n",
        "    !git clone https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook.git\n",
        "    %cd /content/deeplearning-101-audiomir_notebook\n",
        "    !ls\n",
        "\n",
        "    import urllib.request\n",
        "    import shutil\n",
        "    ROOT = 'https://perso.telecom-paristech.fr/gpeeters/tuto_DL101forMIR/'\n",
        "\n",
        "    #hdf5_audio_file, pyjama_annot_file = 'gtzan-genre_audio.hdf5.zip', 'gtzan-genre.pyjama'\n",
        "    hdf5_audio_file, pyjama_annot_file = 'mtt_audio.hdf5.zip', 'mtt.pyjama'\n",
        "    #hdf5_audio_file, pyjama_annot_file = 'rwc-pop_chord_audio.hdf5.zip', 'rwc-pop_chord_audio.pyjama'\n",
        "\n",
        "    urllib.request.urlretrieve(ROOT + hdf5_audio_file, hdf5_audio_file)\n",
        "    if hdf5_audio_file.endswith('.zip'): shutil.unpack_archive(hdf5_audio_file, './')\n",
        "    urllib.request.urlretrieve(ROOT + pyjama_annot_file, pyjama_annot_file)\n",
        "\n",
        "    ROOT = './'\n",
        "\n",
        "else:\n",
        "\n",
        "    ROOT = '/tsi/data_doctorants/gpeeters/_data/'\n",
        "\n",
        "config_file = 'config_autotagging.yaml'\n",
        "#config_file = 'config_chord.yaml'\n",
        "\n",
        "do_wandb = False\n",
        "import pprint as pp\n",
        "import yaml\n",
        "! pip install munch --quiet\n",
        "from munch import munchify\n",
        "with open(config_file, 'r') as fid:\n",
        "    cfg_dic = yaml.safe_load(fid)\n",
        "config = munchify(cfg_dic)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pp.pprint(cfg_dic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvLylAUw4ww9",
        "outputId": "601b65e8-b41b-4628-f5a1-3f6880b9c208"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dataset': {'annot_key': 'tag',\n",
            "             'base': 'mtt',\n",
            "             'n_out': 50,\n",
            "             'problem': 'multilabel'},\n",
            " 'feature': {'L_n': 2048,\n",
            "             'STEP_n': 1024,\n",
            "             'nb_band': 128,\n",
            "             'patch_L_frame': 64,\n",
            "             'patch_STEP_frame': 32,\n",
            "             'type': 'lms'},\n",
            " 'model': {'block_l': [{'sequential_l': [{'layer_l': [['LayerNorm',\n",
            "                                                       {'normalized_shape': -1}],\n",
            "                                                      ['Conv2d',\n",
            "                                                       {'in_channels': 1,\n",
            "                                                        'kernel_size': [128, 5],\n",
            "                                                        'out_channels': 80,\n",
            "                                                        'stride': [1, 1]}],\n",
            "                                                      ['Squeeze', {'dim': [2]}],\n",
            "                                                      ['LayerNorm',\n",
            "                                                       {'normalized_shape': -1}],\n",
            "                                                      ['Activation',\n",
            "                                                       'LeakyReLU'],\n",
            "                                                      ['Dropout', {'p': 0}]]},\n",
            "                                         {'layer_l': [['Conv1d',\n",
            "                                                       {'in_channels': -1,\n",
            "                                                        'kernel_size': 5,\n",
            "                                                        'out_channels': 60,\n",
            "                                                        'stride': 1}],\n",
            "                                                      ['MaxPool1d',\n",
            "                                                       {'kernel_size': 3,\n",
            "                                                        'stride': 3}],\n",
            "                                                      ['LayerNorm',\n",
            "                                                       {'normalized_shape': -1}],\n",
            "                                                      ['Activation',\n",
            "                                                       'LeakyReLU'],\n",
            "                                                      ['Dropout', {'p': 0}]]},\n",
            "                                         {'layer_l': [['Conv1d',\n",
            "                                                       {'in_channels': -1,\n",
            "                                                        'kernel_size': 5,\n",
            "                                                        'out_channels': 60,\n",
            "                                                        'stride': 1}],\n",
            "                                                      ['MaxPool1d',\n",
            "                                                       {'kernel_size': 3,\n",
            "                                                        'stride': 3}],\n",
            "                                                      ['LayerNorm',\n",
            "                                                       {'normalized_shape': -1}],\n",
            "                                                      ['Activation',\n",
            "                                                       'LeakyReLU'],\n",
            "                                                      ['Dropout', {'p': 0}]]},\n",
            "                                         {'layer_l': [['Permute',\n",
            "                                                       {'shape': [0,\n",
            "                                                                  2,\n",
            "                                                                  1]}]]}]},\n",
            "                       {'sequential_l': [{'layer_l': [['LayerNorm',\n",
            "                                                       {'normalized_shape': -1}],\n",
            "                                                      ['Linear',\n",
            "                                                       {'in_features': -1,\n",
            "                                                        'out_features': 128}],\n",
            "                                                      ['BatchNorm1dT',\n",
            "                                                       {'num_features': -1}],\n",
            "                                                      ['Activation',\n",
            "                                                       'LeakyReLU'],\n",
            "                                                      ['Dropout', {'p': 0}]]},\n",
            "                                         {'layer_l': [['Linear',\n",
            "                                                       {'in_features': -1,\n",
            "                                                        'out_features': 128}],\n",
            "                                                      ['BatchNorm1dT',\n",
            "                                                       {'num_features': -1}],\n",
            "                                                      ['Activation',\n",
            "                                                       'LeakyReLU'],\n",
            "                                                      ['Dropout', {'p': 0}]]},\n",
            "                                         {'layer_l': [['Linear',\n",
            "                                                       {'in_features': -1,\n",
            "                                                        'out_features': 128}],\n",
            "                                                      ['BatchNorm1dT',\n",
            "                                                       {'num_features': -1}],\n",
            "                                                      ['Activation',\n",
            "                                                       'LeakyReLU'],\n",
            "                                                      ['Dropout', {'p': 0}]]},\n",
            "                                         {'layer_l': [['Permute',\n",
            "                                                       {'shape': [0, 2, 1]}],\n",
            "                                                      ['UnSqueeze',\n",
            "                                                       {'dim': 2}]]},\n",
            "                                         {'layer_l': [['AutoPoolWeightSplit',\n",
            "                                                       'empty'],\n",
            "                                                      ['Squeeze',\n",
            "                                                       {'dim': [2, 3]}]]}]},\n",
            "                       {'sequential_l': [{'layer_l': [['Linear',\n",
            "                                                       {'in_features': -1,\n",
            "                                                        'out_features': 10}]]}]}],\n",
            "           'name': 'AutoTagging'},\n",
            " 'output_file': {'ext': '_D2-I1-C1.ipynb',\n",
            "                 'origin': 'TUTO_task_Auto_Tagging.ipynb'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a125GFjL4cXn"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w80y-g6n4cXn",
        "outputId": "72b076ad-7f00-429d-8e78-f4772cde8c0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/811.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/890.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.6/890.6 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/815.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "import torchsummary\n",
        "\n",
        "! pip install lightning --quiet\n",
        "import lightning.pytorch as pl\n",
        "from lightning.pytorch.callbacks import Callback\n",
        "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "! pip install wandb --quiet\n",
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "import json\n",
        "import yaml\n",
        "import h5py\n",
        "import pprint as pp\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "from multiprocessing import Pool\n",
        "import os\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [15, 10]\n",
        "import IPython.display\n",
        "\n",
        "# -----------------------------\n",
        "import model_factory\n",
        "import feature\n",
        "import importlib\n",
        "importlib.reload(model_factory)\n",
        "importlib.reload(feature)\n",
        "\n",
        "from argparse import Namespace\n",
        "! pip install munch --quiet\n",
        "from munch import munchify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk3DRWMP4cXn"
      },
      "source": [
        "## Set fixed seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PR9xUK_i4cXn"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "seed = 42\n",
        "torch.manual_seed(seed)         # For CPU\n",
        "random.seed(seed)               # For Python's built-in random module\n",
        "np.random.seed(seed)            # For NumPy\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)          # For current GPU\n",
        "    torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8LOv4X24cXo"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ARLfoPGa4cXo"
      },
      "outputs": [],
      "source": [
        "hdf5_audio_file = f'{ROOT}/{config.dataset.base}_audio.hdf5'\n",
        "pyjama_annot_file = f'{ROOT}/{config.dataset.base}.pyjama'\n",
        "\n",
        "param_model = Namespace()\n",
        "if config.dataset.problem == 'segment':\n",
        "    param_model.batch_size = 32\n",
        "else:\n",
        "    param_model.batch_size = 128\n",
        "param_model.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "param_lightning = Namespace()\n",
        "param_lightning.max_epochs = 500\n",
        "param_lightning.patience = 10\n",
        "param_lightning.dirpath='_autotagging_lighning/'\n",
        "param_lightning.filename='best_model_autotagging'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6OgWTiI4cXo"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W64w5e0p4cXo"
      },
      "source": [
        "### Test loading pyjama/hdf5\n",
        "\n",
        "All the audio data of a dataset are stored in a single [.hdf5](https://docs.h5py.org/) file.\n",
        "Each `key` corresponds to an entry, a specific audiofile.\n",
        "Its array contains the audio wavform and the attribute `sr_hz` provides its sampling rate.\n",
        "\n",
        "All the annotations of a dataset are stored in a single *.pyjama file.\n",
        "As [JAMS](https://github.com/marl/jams) files, .pyjama files are JSON files.\n",
        "However, a single .pyjama file can contain the annotations of ALL entries of a dataset.\n",
        "Its specifications are described here [DOC](https://github.com/geoffroypeeters/pyjama).\n",
        "The values of the `filepath` field of the .pyjama file correspond to the `key` values of the .hdf5 file.\n",
        "\n",
        "We want to solve `multi-class` and `multi-label` problem with the same code. However, in the former we have only one label per item while we can have several for the later. TO deal with this we define the functions `f_get_labelname_dict` and `f_get_groundtruth_item` that provides the list of unique labels and the labels of a given item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "N8pnb0uv4cXp"
      },
      "outputs": [],
      "source": [
        "def f_get_labelname_dict(data_d, annot_key):\n",
        "    \"\"\"\n",
        "    description:\n",
        "        provides the dictionary of labelname used in a given corpus\n",
        "    \"\"\"\n",
        "    labelname_dict_l = []\n",
        "    for entry in data_d['collection']['entry']:\n",
        "        for annot in entry[annot_key]:\n",
        "            labelname_dict_l.append(annot['value'])\n",
        "    labelname_dict_l = list(set(labelname_dict_l))\n",
        "    return sorted(labelname_dict_l)\n",
        "\n",
        "def f_get_groundtruth_item(entry, annot_key, labelname_dict_l, problem, time_sec_v):\n",
        "    \"\"\"\n",
        "    description:\n",
        "        map a label (or a list of labels) to a ground-truth\n",
        "    \"\"\"\n",
        "    if problem=='multiclass':\n",
        "        annot = entry[annot_key][0]\n",
        "        idx_label = labelname_dict_l.index(annot['value'])\n",
        "    elif problem=='multilabel':\n",
        "        idx_label = np.zeros(len(labelname_dict_l))\n",
        "        for annot in entry[annot_key]:\n",
        "            pos = labelname_dict_l.index(annot['value'])\n",
        "            idx_label[ pos ] = 1\n",
        "    elif problem=='segment':\n",
        "        idx_label = np.zeros(time_sec_v.shape, dtype=int)\n",
        "        for segment in entry[annot_key]:\n",
        "            pos = np.where((segment['time'] < time_sec_v) & (time_sec_v <= segment['time'] + segment['duration']))\n",
        "            if pos[0].size > 0:\n",
        "                idx_label[pos] = labelname_dict_l.index(segment['value'])\n",
        "    return idx_label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Nlc7ieN84cXp",
        "outputId": "4b101402-0023-4511-ece4-3c0c563df003"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './/mtt.pyjama'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-91c08ec2fab2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyjama_annot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_fid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mentry_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'collection'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entry'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#pp.pprint(entry_l[0:2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './/mtt.pyjama'"
          ]
        }
      ],
      "source": [
        "with open(pyjama_annot_file, encoding = \"utf-8\") as json_fid:\n",
        "    data_d = json.load(json_fid)\n",
        "entry_l = data_d['collection']['entry']\n",
        "\n",
        "#pp.pprint(entry_l[0:2])\n",
        "\n",
        "\n",
        "audiofile_l = [entry['filepath'][0]['value'] for entry in entry_l]\n",
        "print(f'number of audio: {len(audiofile_l)}')\n",
        "pp.pprint(audiofile_l[:5])\n",
        "\n",
        "\n",
        "labelname_dict_l = f_get_labelname_dict(data_d, config.dataset.annot_key)\n",
        "print(f'number of tags: {len(labelname_dict_l)}')\n",
        "pp.pprint(labelname_dict_l[:5])\n",
        "\n",
        "\n",
        "with h5py.File(hdf5_audio_file, 'r') as hdf5_fid:\n",
        "    #audiofile_l = [key for key in hdf5_fid['/'].keys()]\n",
        "    pp.pprint(f\"audio shape: {hdf5_fid[audiofile_l[0]][:].shape}\")\n",
        "    pp.pprint(f\"audio sample-rate: {hdf5_fid[audiofile_l[0]].attrs['sr_hz']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEDqee1r4cXp"
      },
      "outputs": [],
      "source": [
        "if config.dataset.problem=='segment':\n",
        "    print( data_d['collection']['entry'][0] )\n",
        "    labelname_dict_l = f_get_labelname_dict(data_d, config.dataset.annot_key)\n",
        "    print(len(labelname_dict_l))\n",
        "    time_sec_v = np.arange(0,205,0.1)\n",
        "    idx_label = f_get_groundtruth_item(data_d['collection']['entry'][0], config.dataset.annot_key, labelname_dict_l, config.dataset.problem, time_sec_v)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 1, figsize=(10, 6))\n",
        "    axes.plot(time_sec_v, idx_label)\n",
        "    axes.set_xlim(0,10)\n",
        "    axes.set_yticks(np.arange(0,len(labelname_dict_l)))\n",
        "    axes.set_yticklabels(labelname_dict_l)\n",
        "    axes.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqL-QWdY4cXp"
      },
      "source": [
        "### Define features\n",
        "\n",
        "We define the feature that will be used as input to the model, the `X`, and test it on an audio file.\n",
        "We will use here Log-Mel-Spectrogram provided by the function `feature.f_get_lms`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3ecWXuZ4cXp"
      },
      "outputs": [],
      "source": [
        "# --- TEST\n",
        "idx_file = 50\n",
        "with h5py.File(hdf5_audio_file, 'r') as hdf5_fid:\n",
        "    print(audiofile_l[idx_file])\n",
        "    audio_v = hdf5_fid[audiofile_l[idx_file]][:]\n",
        "    sr_hz = hdf5_fid[audiofile_l[idx_file]].attrs['sr_hz']\n",
        "\n",
        "if config.feature.type == 'waveform':\n",
        "    data_m, time_sec_v = feature.f_get_waveform(audio_v, sr_hz)\n",
        "elif config.feature.type == 'lms':\n",
        "    data_m, time_sec_v = feature.f_get_lms(audio_v, sr_hz, config.feature)\n",
        "    data_m = data_m.squeeze(0)\n",
        "elif config.feature.type == 'hcqt':\n",
        "    data_m, time_sec_v, frequency_hz_v = feature.f_get_hcqt(audio_v, sr_hz, config.feature)\n",
        "    data_m = data_m.squeeze(0)\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "print(data_m.shape)\n",
        "plt.imshow(data_m, origin='lower', aspect='auto'); plt.colorbar();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGW7DvLL4cXp"
      },
      "source": [
        "#### Defining patches\n",
        "\n",
        "The input `X` of our model is actually a temporal patch/chunk of the whole feature matrix `(nb_time, nb_dim)`.\n",
        "For a given feature matrix of length `total_len`, the function `feature.f_get_patches` provides the list of possible patches of length `patch_len` and hop size `patch_hopsize` (we perform frame-analysis over the feature matrix).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ33jJcR4cXq"
      },
      "outputs": [],
      "source": [
        "# --- TEST\n",
        "feature.f_get_patches(total_len=250, patch_len=64, patch_hopsize=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MXAlJz_4cXq"
      },
      "source": [
        "### Create Dataset\n",
        "\n",
        "The class `TagDataset` (a subset of pytorch `Dataset` class) is responsible for providing (with the `__getitem` method) the input `X` and ground-truth `y` of the pitches existing in a patch.\n",
        "\n",
        "- `__getitem__`: for a given `idx_patch`, it provides the `X` (either a patch of CQT or Harmonic-CQT) tensor and the ground-truth binary pitch matrix\n",
        "- '__init__':\n",
        "  - will read the hdf5_feat_file and pyjama_annot_file\n",
        "  - split their content according to training and test\n",
        "  - convert the tag annotations to the format expected for `y`\n",
        "  - store all the necessary data in memory (of CPU) to fasten later access: `self.data_d[key=audiofile]` for the CQT/H-CQT, `self.patch_l` for the list of all possible patch over all possibe audiofile. The `__getitem__` method simply get a patch, which audiofile it is coming from, and its position within it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoLCU8tm4cXq"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class TagDataset(Dataset):\n",
        "    \"\"\"\n",
        "    description\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hdf5_audio_file, pyjama_annot_file, do_train):\n",
        "\n",
        "        with open(pyjama_annot_file, encoding = \"utf-8\") as json_fid: data_d = json.load(json_fid)\n",
        "        entry_l = data_d['collection']['entry']\n",
        "\n",
        "        self.labelname_dict_l = f_get_labelname_dict(data_d, config.dataset.annot_key)\n",
        "\n",
        "        self.do_train = do_train\n",
        "        if self.do_train:   entry_l = [entry_l[idx] for idx in range(len(entry_l)) if (idx % 5) != 0]\n",
        "        else:               entry_l = [entry_l[idx] for idx in range(len(entry_l)) if (idx % 5) == 0]\n",
        "\n",
        "        self.audio_file_l =  [entry['filepath'][0]['value'] for entry in entry_l]\n",
        "\n",
        "        self.data_d = {}\n",
        "        self.patch_l = []\n",
        "\n",
        "        with h5py.File(hdf5_audio_file, 'r') as audio_fid:\n",
        "            for idx_entry, entry in enumerate(tqdm(entry_l)):\n",
        "                audio_file= entry['filepath'][0]['value']\n",
        "\n",
        "                # --- get features\n",
        "                if config.feature.type == 'waveform':\n",
        "                    #feat_value_m, time_sec_v = feature.f_get_waveform(audio_v, sr_hz)\n",
        "                    feat_value_m = audio_fid[audio_file][:].reshape(1,-1)\n",
        "                    time_sec_v = np.arange(0,2)/audio_fid[audio_file].attrs['sr_hz']\n",
        "                elif config.feature.type == 'lms':\n",
        "                    audio_v, sr_hz =  audio_fid[audio_file][:], audio_fid[audio_file].attrs['sr_hz']\n",
        "                    feat_value_m, time_sec_v = feature.f_get_lms(audio_v, sr_hz, config.feature)\n",
        "                elif config.feature.type == 'hcqt':\n",
        "                    audio_v, sr_hz =  audio_fid[audio_file][:], audio_fid[audio_file].attrs['sr_hz']\n",
        "                    feat_value_m, time_sec_v, frequency_hz_v = feature.f_get_hcqt(audio_v, sr_hz, config.feature)\n",
        "\n",
        "                if idx_entry==0:\n",
        "                    print(f'A patch corresponds to {np.mean(np.diff(time_sec_v))*config.feature.patch_L_frame} sec.')\n",
        "\n",
        "                # --- map annotations\n",
        "                idx_label = f_get_groundtruth_item(entry, config.dataset.annot_key, self.labelname_dict_l, config.dataset.problem, time_sec_v)\n",
        "\n",
        "                # --- store for later use\n",
        "                self.data_d[audio_file] = {'X': torch.tensor(feat_value_m).float(), 'y': torch.tensor(idx_label)}\n",
        "\n",
        "                # --- create list of patches and associate information\n",
        "                localpatch_l = feature.f_get_patches(feat_value_m.shape[-1], config.feature.patch_L_frame, config.feature.patch_STEP_frame)\n",
        "                for localpatch in localpatch_l:\n",
        "                    self.patch_l.append({'audiofile': audio_file,\n",
        "                                        'start_frame': localpatch['start_frame'],\n",
        "                                        'end_frame': localpatch['end_frame'],\n",
        "                                        })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patch_l)\n",
        "\n",
        "    def __getitem__(self, idx_patch):\n",
        "        audiofile = self.patch_l[idx_patch]['audiofile']\n",
        "        s = self.patch_l[idx_patch]['start_frame']\n",
        "        e = self.patch_l[idx_patch]['end_frame']\n",
        "\n",
        "        if config.feature.type == 'waveform':\n",
        "            # --- X is (C, nb_time)\n",
        "            X = self.data_d[ audiofile ]['X'][:,s:e]\n",
        "        elif config.feature.type in ['lms', 'hcqt']:\n",
        "            # --- X is (C, nb_dim, nb_time)\n",
        "            X = self.data_d[ audiofile ]['X'][:,:,s:e]\n",
        "\n",
        "        if config.dataset.problem in ['multiclass', 'multilabel']:\n",
        "            y = self.data_d[ audiofile ]['y'] # --- We suppose the same annotation for the whole file\n",
        "        else:\n",
        "            y = self.data_d[ audiofile ]['y'][s:e] # --- Segment\n",
        "        return {'X':X , 'y':y}\n",
        "\n",
        "train_dataset = TagDataset(hdf5_audio_file, pyjama_annot_file, do_train=True)\n",
        "valid_dataset = TagDataset(hdf5_audio_file, pyjama_annot_file, do_train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvxiiWOY4cXq"
      },
      "outputs": [],
      "source": [
        "# --- TEST\n",
        "print(len(train_dataset))\n",
        "print(train_dataset[0]['X'].shape, train_dataset[0]['X'].dtype)\n",
        "print(train_dataset[0]['y'].shape, train_dataset[0]['y'].dtype)\n",
        "\n",
        "idx = 20\n",
        "plt.figure(figsize=(10,4));\n",
        "if config.feature.type == 'waveform':\n",
        "    plt.subplot(121); plt.plot(train_dataset[idx]['X'].squeeze(0));\n",
        "elif config.feature.type in ['lms', 'hcqt']:\n",
        "    plt.subplot(121); plt.imshow(train_dataset[idx]['X'].squeeze(0), aspect='auto', origin='lower'); plt.colorbar();\n",
        "if config.dataset.problem == 'segment':\n",
        "    plt.subplot(122); plt.plot(train_dataset[idx]['y']);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZqoT7jD4cXr"
      },
      "source": [
        "### Create DataLoader\n",
        "\n",
        "We create the dataloader for the training and validation data from the corresponding dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYTMYFI04cXr"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=param_model.batch_size, shuffle=True, num_workers=8, drop_last = True)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=param_model.batch_size, shuffle=False, num_workers=8, drop_last = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMiICZVL4cXr"
      },
      "outputs": [],
      "source": [
        "# --- TEST\n",
        "batch = next(iter(train_dataloader))\n",
        "print(batch['X'].size())\n",
        "print(batch['y'].size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn4m3B7t4cXr"
      },
      "source": [
        "## Model\n",
        "\n",
        "The pytorch model is generated automatically based on the content of the `config_autotagging.yaml` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wY15F7hK4cXr"
      },
      "outputs": [],
      "source": [
        "importlib.reload(model_factory)\n",
        "with open(config_file, 'r') as fid: cfg_dic = yaml.safe_load(fid)\n",
        "config = munchify(cfg_dic)\n",
        "\n",
        "config.model.block_l[-1].sequential_l[-1].layer_l[-1][1].out_features  = config.dataset.n_out\n",
        "\n",
        "if config.feature.type == 'waveform':\n",
        "    m, C, T = param_model.batch_size, 1, config.feature.patch_L_frame\n",
        "    model = model_factory.NetModel(config, [m, C, T])\n",
        "if config.feature.type in ['lms', 'hcqt']:\n",
        "    m, C, H, W = param_model.batch_size, 1, config.feature.nb_band, config.feature.patch_L_frame\n",
        "    model = model_factory.NetModel(config, [m, C, H, W])\n",
        "\n",
        "model = model.to(param_model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgBXyWBt4cXr"
      },
      "source": [
        "### Check the model\n",
        "\n",
        "We can check the model either using\n",
        "- print(model)\n",
        "- the `verbose` mode of the `forward` of the model\n",
        "- the `torchsummary.summary` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxGkmdJ94cXr"
      },
      "outputs": [],
      "source": [
        "# --- TEST\n",
        "if config.feature.type == 'waveform':\n",
        "    m, C, T = param_model.batch_size, 1, config.feature.patch_L_frame\n",
        "\n",
        "    X = torch.randn(m, C, T).to(param_model.device)\n",
        "    print(model(X, True).size())\n",
        "\n",
        "    #torchsummary.summary(model, input_size=(C, T))\n",
        "\n",
        "if config.feature.type in ['lms', 'hcqt']:\n",
        "    m, C, H, W = param_model.batch_size, 1, config.feature.nb_band, config.feature.patch_L_frame\n",
        "\n",
        "    X = torch.randn(m, C, H, W).to(param_model.device)\n",
        "    print(model(X, True).size())\n",
        "\n",
        "    #torchsummary.summary(model, input_size=(C, H, W))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "884vicEc4cXr"
      },
      "source": [
        "### Test the model\n",
        "\n",
        "We can also test the model and the loss, i.e. checking that the format of the `X` corresponds to what the model is expected; and the format of the `y` and `hat_y` corresponds to what the loss is expected.\n",
        "If this is OK we can start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSQ4syJH4cXs"
      },
      "outputs": [],
      "source": [
        "model.to(param_model.device)\n",
        "batch = next(iter(train_dataloader))\n",
        "X = batch['X'].to(param_model.device)\n",
        "y = batch['y'].to(param_model.device)\n",
        "hat_y = model(X)\n",
        "print(f'{X.size()} {y.size()} {hat_y.squeeze(1).size()}')\n",
        "print(f'{X.dtype} {y.dtype} {hat_y.squeeze(1).dtype}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82h3dZEz4cXs"
      },
      "outputs": [],
      "source": [
        "def do_reshape_segment(hat_y, y, C=25):\n",
        "    \"\"\"\n",
        "    reshape in case of sequences of labels\n",
        "    \"\"\"\n",
        "    # --- hat_y (batch_size, C, len_sequence) -> (batch_size, len_sequence, C)\n",
        "    hat_y = hat_y.permute(0, 2, 1).contiguous()\n",
        "    return hat_y.reshape(-1, C), y.reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9wrYvbf4cXs"
      },
      "outputs": [],
      "source": [
        "if config.dataset.problem == 'multiclass':\n",
        "    nn.CrossEntropyLoss()(hat_y, y)\n",
        "elif config.dataset.problem == 'segment':\n",
        "    hat_y , y = do_reshape_segment(hat_y, y)\n",
        "    print(f'{y.size()} {hat_y.squeeze(1).size()}')\n",
        "    nn.CrossEntropyLoss()(hat_y, y)\n",
        "elif config.dataset.problem=='multilabel':\n",
        "    nn.BCEWithLogitsLoss(reduction='none')(hat_y.squeeze(1), y).size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HFtpS3E4cXs"
      },
      "source": [
        "## Training using TorchLightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmOzosW14cXs"
      },
      "source": [
        "### TorchLightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7SvPQLp4cXs"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(hat_y_prob, y_idx):\n",
        "    \"\"\" Manually compute accuracy \"\"\"\n",
        "    hat_y_idx = torch.argmax(hat_y_prob, dim=1)  # Get the predicted class (index of max logit)\n",
        "    correct = (hat_y_idx == y_idx).float()  # Compare with ground truth and cast to float\n",
        "    accuracy = correct.sum() / len(correct)  # Compute mean accuracy over the batch\n",
        "    return accuracy\n",
        "\n",
        "class AutoTaggingLigthing(pl.LightningModule):\n",
        "    def __init__(self, in_model):\n",
        "        super().__init__()\n",
        "        self.model = in_model\n",
        "        if config.dataset.problem=='multiclass':    self.loss = nn.CrossEntropyLoss()\n",
        "        elif config.dataset.problem == 'segment':   self.loss = nn.CrossEntropyLoss()\n",
        "        elif config.dataset.problem=='multilabel':  self.loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        hat_y_prob = self.model(batch['X'])\n",
        "        if config.dataset.problem == 'segment': hat_y_prob, y_idx = do_reshape_segment(hat_y_prob, batch['y'])\n",
        "        else: y_idx = batch['y']\n",
        "\n",
        "        loss = self.loss(hat_y_prob, y_idx)\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        if config.dataset.problem in ['multiclass', 'segment']:\n",
        "            accuracy = get_accuracy(hat_y_prob, y_idx)\n",
        "            self.log('train_acc', accuracy, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        hat_y_prob = self.model(batch['X'])\n",
        "        if config.dataset.problem == 'segment': hat_y_prob, y_idx = do_reshape_segment(hat_y_prob, batch['y'])\n",
        "        else: y_idx = batch['y']\n",
        "\n",
        "        loss = self.loss(hat_y_prob, y_idx)\n",
        "\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        if config.dataset.problem==['multiclass', 'segment']:\n",
        "            accuracy = get_accuracy(hat_y_prob, y_idx)\n",
        "            self.log('val_acc', accuracy, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), 0.01)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfaflUFn4cXs"
      },
      "source": [
        "### Training\n",
        "\n",
        "We train the model and apply a early-stopping based on the validation loss. We also monitor the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29poODuS4cXt"
      },
      "outputs": [],
      "source": [
        "my_lighting = AutoTaggingLigthing( model )\n",
        "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=param_lightning.patience, verbose=True, mode=\"min\")\n",
        "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath=param_lightning.dirpath, filename=param_lightning.filename, save_top_k=1, mode='min')\n",
        "trainer = pl.Trainer(accelerator=\"gpu\",  max_epochs = param_lightning.max_epochs, callbacks = [early_stop_callback, checkpoint_callback])\n",
        "trainer.fit(model=my_lighting, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dzRelaD4cXt"
      },
      "outputs": [],
      "source": [
        "if config.dataset.problem == 'segment':\n",
        "    batch = next(iter(train_dataloader))\n",
        "\n",
        "    model = model.to(param_model.device)\n",
        "    X = batch['X'].to(param_model.device)\n",
        "    y_idx = batch['y'].to(param_model.device)\n",
        "    hat_y_prob = model(X)\n",
        "\n",
        "    idx = 1\n",
        "    plt.figure(figsize=(12,6));\n",
        "    plt.subplot(121);\n",
        "    plt.imshow(X[idx].cpu().numpy().squeeze(0), aspect='auto', origin='lower'); plt.xlabel('Time')\n",
        "\n",
        "    axes = plt.subplot(122);\n",
        "    axes.imshow(hat_y_prob[idx].detach().cpu().numpy(), aspect='auto', origin='lower', interpolation=None); plt.xlabel('Time')\n",
        "    axes.plot(y_idx[idx].cpu().numpy(), 'w', label='ground-truth');\n",
        "    axes.plot(hat_y_prob[idx].argmax(axis=0).detach().cpu().numpy(), 'r--', label='predicted')\n",
        "    axes.legend()\n",
        "    axes.set_yticks(np.arange(0,len(valid_dataset.labelname_dict_l)))\n",
        "    axes.set_yticklabels(valid_dataset.labelname_dict_l)\n",
        "    axes.grid(True)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F24e9rv4cXt"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We first load the best model obtained during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6l2ykbV4cXt"
      },
      "outputs": [],
      "source": [
        "best_model_path = checkpoint_callback.best_model_path\n",
        "print(f\"Best model saved at: {best_model_path}\")\n",
        "my_lighting = AutoTaggingLigthing.load_from_checkpoint(best_model_path, in_model=model)\n",
        "\n",
        "print( type(model) )\n",
        "print( type(my_lighting) )\n",
        "print( type(my_lighting.model) )\n",
        "\n",
        "model = my_lighting.model\n",
        "model.to(param_model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRmUD0NI4cXt"
      },
      "source": [
        "### Multi-class/Segment/Multi-label: performance measures using scikitlearn\n",
        "\n",
        "We send all the data of the validation dataset to the model, get the estimated pitch `hat_y`.\n",
        "\n",
        "- For **multiclass**, we then choose the most-likely class for each item `np.argmax` and compute the `recall`, `precision`, `f-measure`, `accuracy`.\n",
        "- For **segment**, we do the same after concatenating all decisions over time of a given patch\n",
        "- For **multilabel**, we preserve the probability outputs, and evaluate the results for various possible threshold using the `roc_auc_score` and `average_precision_score`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX7N-iiz4cXu"
      },
      "outputs": [],
      "source": [
        "if config.dataset.problem in ['multiclass', 'segment']:\n",
        "    y_idx_l = []\n",
        "    hat_y_idx_l = []\n",
        "    for batch in valid_dataloader:\n",
        "        hat_y_prob = model(batch['X'].to(param_model.device))\n",
        "        if config.dataset.problem == 'segment': hat_y_prob, y = do_reshape_segment(hat_y_prob, batch['y'])\n",
        "        else: y = batch['y']\n",
        "        y_idx_l.append(y)\n",
        "        hat_y_idx_l.append(np.argmax(hat_y_prob.detach().cpu().numpy(), axis=1))\n",
        "\n",
        "    y_idx_v = np.concatenate(y_idx_l)\n",
        "    hat_y_idx_v = np.concatenate(hat_y_idx_l)\n",
        "    print(y_idx_v.shape)\n",
        "    print(hat_y_idx_v.shape)\n",
        "\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    classification_reports = classification_report(y_true=y_idx_v, y_pred=hat_y_idx_v, output_dict=True, zero_division=0)\n",
        "    pp.pprint(classification_reports['macro avg'])\n",
        "    cm = confusion_matrix(y_true=y_idx_v, y_pred=hat_y_idx_v)\n",
        "    print(cm)\n",
        "    # --- accuracy: np.sum(np.diag(cm))/np.sum(cm)\n",
        "    # --- recall: for c in range(10): print(cm[c,c]/np.sum(cm[c,:]))\n",
        "\n",
        "elif config.dataset.problem=='multilabel':\n",
        "    y_ohe_l = []\n",
        "    hat_y_prob_l = []\n",
        "    for batch in valid_dataloader:\n",
        "        hat_y_prob = F.sigmoid(model(batch['X'].to(param_model.device)))\n",
        "        if config.dataset.problem=='multiclass':       y_ohe_l.append(F.one_hot(batch['y'], 10).numpy())\n",
        "        elif config.dataset.problem=='multilabel':     y_ohe_l.append(batch['y'].numpy())\n",
        "        hat_y_prob_l.append(hat_y_prob.detach().cpu().numpy())\n",
        "    y_ohe_m = np.concatenate(y_ohe_l) # --- convert list to array\n",
        "    hat_y_prob_m = np.concatenate(hat_y_prob_l)\n",
        "\n",
        "    # --- remove classes that do not appear in the validation set\n",
        "    mask = np.sum(y_ohe_m, axis=0)>0\n",
        "    y_ohe_m = y_ohe_m[:,mask]\n",
        "    hat_y_prob_m = hat_y_prob_m[:,mask]\n",
        "\n",
        "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "    auc = roc_auc_score(y_true=y_ohe_m, y_score=hat_y_prob_m, average=\"macro\")\n",
        "    print(f'auc: {auc}')\n",
        "    average_precision = average_precision_score(y_true=y_ohe_m, y_score=hat_y_prob_m, average=\"macro\")\n",
        "    print(f'average_precision: {average_precision}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YobCMBuq4cXu"
      },
      "source": [
        "### Illustration: tag-o-gram\n",
        "\n",
        "We illustrate the results obtained by the trained model on a given audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ELJEjE34cXu"
      },
      "outputs": [],
      "source": [
        "def F_tag_o_gram(model, audio_v, sr_hz):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    # --- Compute the audio features\n",
        "    if config.feature.type == 'waveform':    feat_value_m, _ = feature.f_get_waveform(audio_v, sr_hz)\n",
        "    elif config.feature.type == 'lms':       feat_value_m, _ = feature.f_get_lms(audio_v, sr_hz, config.feature)\n",
        "    elif config.feature.type == 'hcqt':      feat_value_m, _, _ = feature.f_get_hcqt(audio_v, sr_hz, config.feature)\n",
        "\n",
        "    # --- Split matrix into patches\n",
        "    nb_frame = feat_value_m.shape[1]\n",
        "    # --- WARNING: we remove overlap between patches by setting STEP_frame=L_frame\n",
        "    patch_info_l = feature.f_get_patches(feat_value_m.shape[-1], config.feature.patch_L_frame, config.feature.patch_L_frame)\n",
        "    nb_patch = len(patch_info_l)\n",
        "\n",
        "    data_l = []\n",
        "    for patch_info in patch_info_l:\n",
        "        if config.feature.type == 'waveform':\n",
        "            data_l.append(feat_value_m[:, patch_info['start_frame']:patch_info['end_frame']])\n",
        "        elif config.feature.type in ['lms', 'hcqt']:\n",
        "            data_l.append(feat_value_m[:, :, patch_info['start_frame']:patch_info['end_frame']])\n",
        "\n",
        "    data_m = np.asarray(data_l)\n",
        "    # --- Convert numpy to torch tensor\n",
        "    X = torch.from_numpy(data_m).float().to(param_model.device)\n",
        "\n",
        "    # --- Get prediction from model\n",
        "    model.eval()\n",
        "    hat_y_prob = model(X)\n",
        "    print(hat_y_prob.shape)\n",
        "    if config.dataset.problem == 'segment':\n",
        "        hat_y_prob = hat_y_prob.permute(0, 2, 1).contiguous().reshape(-1, 25)\n",
        "\n",
        "    if config.dataset.problem in ['multiclass', 'segment']: hat_y_prob = F.softmax(hat_y_prob, dim=1)\n",
        "    elif config.dataset.problem=='multilabel':     hat_y_prob = F.sigmoid(hat_y_prob)\n",
        "    # --- Convert from torch tensor to numpy\n",
        "    hat_y_prob_np = hat_y_prob.cpu().detach().numpy()\n",
        "\n",
        "    fig, axes = plt.subplots(1, 1, figsize=(16, 6))\n",
        "    im = axes.imshow(hat_y_prob_np.T, aspect='auto', interpolation=None, cmap=plt.get_cmap('inferno'))\n",
        "    axes.set_yticks(np.arange(0,len(valid_dataset.labelname_dict_l)))\n",
        "    axes.set_yticklabels(valid_dataset.labelname_dict_l)\n",
        "    axes.grid(True)\n",
        "    fig.colorbar(im, orientation='vertical')\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# ---------------------------------------------\n",
        "# ---------------------------------------------\n",
        "audio_file= valid_dataset.audio_file_l[10]\n",
        "\n",
        "with h5py.File(hdf5_audio_file, 'r') as audio_fid:\n",
        "    audio_value_v = audio_fid[audio_file][:]\n",
        "    audio_sr_hz = audio_fid[audio_file].attrs['sr_hz']\n",
        "F_tag_o_gram(model, audio_value_v, audio_sr_hz)\n",
        "plt.title(audio_file)\n",
        "IPython.display.Audio(data=audio_value_v, rate=audio_sr_hz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUxMLKQn4cXu"
      },
      "outputs": [],
      "source": [
        "print(f'cp {config.output_file.origin} {config.output_file.origin + config.output_file.ext}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayct4Iux4cXu"
      },
      "source": [
        "### Display learned filters Conv1D/ SincNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vedYc9Ax4cXu"
      },
      "outputs": [],
      "source": [
        "# --- Disaply Conv1D filter\n",
        "#named_modules_dict = dict(model.named_modules())\n",
        "#weight = named_modules_dict['model.0'][0][1].weight # --- Conv1D\n",
        "\n",
        "\n",
        "# --- Display SincNet filters\n",
        "named_modules_dict = dict(model.named_modules())\n",
        "weight = named_modules_dict['model.0'][0][1].filters # --- SincNet\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "nb_filter = weight.size()[0]\n",
        "for num_filter in range(nb_filter):\n",
        "    value = weight[num_filter,:,:].squeeze().detach().cpu()\n",
        "    plt.subplot(nb_filter, 2, 2*num_filter+1)\n",
        "    plt.plot( value ); plt.title(num_filter)\n",
        "    plt.subplot(nb_filter, 2, 2*num_filter+2)\n",
        "    plt.plot(np.abs(np.fft.rfft(value)) ); plt.title(num_filter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN8rHREL4cXv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}