{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Install necessary libraries (run this cell first)\n",
    "!pip install music2latent torch librosa soundfile"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9446d307a137c489"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from music2latent import EncoderDecoder\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.signal"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "970f5b0951955d35"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Define the Dataset Class\n",
    "# Define the dataset class\n",
    "class MusicLatentDataset(Dataset):\n",
    "    def __init__(self, root_dir, encoder, extensions=[\".wav\", \".mp3\", \".flac\"]):\n",
    "        self.root_dir = root_dir\n",
    "        self.encoder = encoder\n",
    "        self.extensions = extensions\n",
    "        self.audio_files = []\n",
    "        for root, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if any(file.endswith(ext) for ext in self.extensions):\n",
    "                    self.audio_files.append(os.path.join(root, file))\n",
    "        self.latent_data = [self._encode_audio(file) for file in tqdm(self.audio_files, desc=\"Encoding Audio\")]\n",
    "        # calculate mean and variance of each latent dimension\n",
    "        latent_data = torch.hstack(self.latent_data)\n",
    "        self.mean = latent_data.mean(dim=1)\n",
    "        self.std = latent_data.std(dim=1)\n",
    "        # standardize self.latent_data\n",
    "        self.latent_data = [((latent.permute(1, 0) - self.mean) / self.std).permute(1, 0) for latent in self.latent_data]\n",
    "        torch.save({'mean': self.mean, 'std': self.std}, 'mean_std.pth')\n",
    "        # load\n",
    "        mean_std = torch.load('mean_std.pth')\n",
    "        self.mean = mean_std['mean']\n",
    "        self.std = mean_std['std']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latent_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        latent = self.latent_data[idx]\n",
    "        return latent.float()\n",
    "\n",
    "    def _encode_audio(self, filename):\n",
    "        # Load the audio file using soundfile\n",
    "        waveform, sample_rate = sf.read(filename)\n",
    "\n",
    "        # Ensure it's in float32 precision\n",
    "        waveform = waveform.astype('float32')\n",
    "\n",
    "        # Resample to 44100 Hz if necessary\n",
    "        if sample_rate != 44100:\n",
    "            num_samples = int(len(waveform) * 44100 / sample_rate)\n",
    "            waveform = scipy.signal.resample(waveform, num_samples)\n",
    "            sample_rate = 44100\n",
    "\n",
    "        # Encode using music2latent\n",
    "        latent = self.encoder.encode(waveform)\n",
    "\n",
    "        # Ensure latent is in float32\n",
    "        latent = latent.float()\n",
    "        \n",
    "        # Remove batch dimension if necessary\n",
    "        latent = latent.squeeze(0)\n",
    "\n",
    "        return latent[..., :32]\n",
    "\n",
    "    def unnormalize(self, latent):\n",
    "        return latent * self.std + self.mean"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "236006a7d090b3bb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Define the Diffusion U-Net Model\n",
    "\n",
    "class DiffusionUnet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_layers=3,\n",
    "        base_channels=128,\n",
    "        time_embedding_size=128,\n",
    "    ):\n",
    "        super(DiffusionUnet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Store channels at each layer to match encoder and decoder\n",
    "        self.channels = []\n",
    "\n",
    "        # Downsampling path\n",
    "        self.down_layers = nn.ModuleList()\n",
    "        in_channels_current = in_channels\n",
    "        for i in range(num_layers):\n",
    "            out_channels_current = base_channels * (2 ** i)\n",
    "            conv = nn.Conv1d(\n",
    "                in_channels_current,\n",
    "                out_channels_current,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            )\n",
    "            self.down_layers.append(conv)\n",
    "            self.channels.append(out_channels_current)\n",
    "            in_channels_current = out_channels_current\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Conv1d(\n",
    "            in_channels_current, in_channels_current, kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, time_embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_embedding_size, in_channels_current),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Upsampling path\n",
    "        self.up_layers = nn.ModuleList()\n",
    "        for i in reversed(range(num_layers)):\n",
    "            out_channels_current = in_channels_current // 2\n",
    "            upsample = nn.ConvTranspose1d(\n",
    "                in_channels_current,\n",
    "                out_channels_current,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            )\n",
    "            self.up_layers.append(upsample)\n",
    "            in_channels_current = out_channels_current  # Update for next layer\n",
    "\n",
    "        # Final output layer\n",
    "        self.final_layer = nn.Conv1d(\n",
    "            in_channels_current, out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Ensure input is float32\n",
    "        x = x.float()\n",
    "        t = t.float()\n",
    "\n",
    "        # Downsampling path\n",
    "        skip_connections = []\n",
    "        h = x\n",
    "        for down in self.down_layers:\n",
    "            h = F.relu(down(h))\n",
    "            skip_connections.append(h)\n",
    "\n",
    "        # Bottleneck\n",
    "        h = self.bottleneck(h)\n",
    "\n",
    "        # Time embedding\n",
    "        t_emb = self.time_mlp(t.unsqueeze(-1))  # [batch_size, channels]\n",
    "        t_emb = t_emb.unsqueeze(-1)             # [batch_size, channels, 1]\n",
    "        h = h + t_emb                           # Broadcast addition\n",
    "\n",
    "        # Upsampling path\n",
    "        for up in self.up_layers:\n",
    "            skip = skip_connections.pop()\n",
    "            h = h + skip  # Element-wise addition of skip connection\n",
    "            h = F.relu(up(h))\n",
    "\n",
    "        # Final output layer\n",
    "        out = self.final_layer(h)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50118cc78fc0a48b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Define the RectifiedFlows Class\n",
    "\n",
    "# Define the RectifiedFlows class\n",
    "class RectifiedFlows(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 sigma_data=1,\n",
    "                 # Expected mean and standard deviation of the training data.\n",
    "                 P_mean=0.,\n",
    "                 P_std=1.\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.sigma_data = sigma_data\n",
    "        self.P_std = P_std\n",
    "        self.P_mean = P_mean\n",
    "\n",
    "    def add_noise(self, x, noise, times):\n",
    "        if isinstance(times, int):\n",
    "            times = float(times)\n",
    "        if isinstance(times, float):\n",
    "            times = torch.ones((x.shape[0],), dtype=x.dtype, device=x.device) * times\n",
    "        if len(times.shape) == 1:\n",
    "            shape = [times.shape[0]] + (x.ndim - 1)*[1]\n",
    "            times = times.reshape(shape)\n",
    "        elif len(times.shape) == 2:\n",
    "            shape = [times.shape[0]]+ [1]*(x.ndim - 2) + [-1]\n",
    "            times = times.reshape(shape)\n",
    "        return (1. - times) * x + times * noise\n",
    "\n",
    "    def forward(self, model, y, sigma=None, return_loss=True,\n",
    "                **model_kwargs) -> torch.Tensor:\n",
    "        y = y.float() * self.sigma_data  # Ensure y is float32\n",
    "        times_length = y.size(-1)\n",
    "        times = torch.nn.functional.sigmoid(\n",
    "            torch.randn(y.shape[0], dtype=torch.float32,\n",
    "                        device=y.device) * self.P_std + self.P_mean\n",
    "        )\n",
    "        noises = torch.randn_like(y)\n",
    "        v = y - noises\n",
    "        noisy_samples = self.add_noise(y, noises, times)\n",
    "        fv = model(\n",
    "            noisy_samples,\n",
    "            times,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        mse = nn.MSELoss()\n",
    "        loss = mse(v, fv)\n",
    "        if return_loss:\n",
    "            return {'loss': loss}, (fv + noises) / self.sigma_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf856541bccee0f3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Define the Inference Function\n",
    "def inference(rectified_flows, net, latents_shape, num_steps):\n",
    "    sigma_data = rectified_flows.sigma_data\n",
    "    dtype = torch.float32  # Ensuring float32\n",
    "    # Adjust noise levels based on what's supported by the network.\n",
    "    step_size = 1 / num_steps\n",
    "    current_sample = torch.randn(latents_shape, dtype=dtype, device=next(net.parameters()).device)\n",
    "    times = torch.ones(latents_shape[0], dtype=dtype, device=current_sample.device)\n",
    "    for i in tqdm(range(num_steps), desc=\"Sampling\", leave=False):\n",
    "        v = net(\n",
    "            current_sample,\n",
    "            times\n",
    "        )\n",
    "        current_sample = current_sample + step_size * v\n",
    "        times = times - step_size\n",
    "    return current_sample / sigma_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24b666d971ef82f6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Initialize the Encoder/Decoder and Datasets\n",
    "# Initialize the encoder/decoder\n",
    "encdec = EncoderDecoder()\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "audio_folder_train = \"./NSYNTH/nsynth-train\"\n",
    "audio_folder_val = \"./NSYNTH/nsynth-valid\"\n",
    "\n",
    "dataset = MusicLatentDataset(root_dir=audio_folder_train, encoder=encdec)\n",
    "dataloader = DataLoader(dataset, batch_size=500, shuffle=True)\n",
    "\n",
    "dataset_val = MusicLatentDataset(root_dir=audio_folder_val, encoder=encdec)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=500, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc83d1048f720729"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Initialize the Model\n",
    "# Initialize the model\n",
    "in_channels = 64  # latent dimension\n",
    "out_channels = 64  # output dimension\n",
    "model = DiffusionUnet(in_channels, out_channels, num_layers=5).cuda()\n",
    "# torch.Size([1, 128, 16])\n",
    "# torch.Size([1, 256, 8])\n",
    "# torch.Size([1, 512, 4])\n",
    "# torch.Size([1, 1024, 2])\n",
    "# torch.Size([1, 2048, 1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b71b7b1d08e1b620"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Optionally Load a Pre-Trained Model\n",
    "model.load_state_dict(torch.load('model_diffusion.pth'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d172ed80c30c023e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize RectifiedFlows\n",
    "rectified_flows = RectifiedFlows().cuda()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "984ff2d0e98f0663"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Training and Validation loop\n",
    "\n",
    "epochs = 5000\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "        y = batch.cuda().float()  # Ensure y is float32\n",
    "        # Forward pass\n",
    "        loss_dict, _ = rectified_flows(model, y)\n",
    "        loss = loss_dict['loss']\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}')\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'model_diffusion.pth')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader_val, desc=f\"Validation Epoch {epoch + 1}/{epochs}\", leave=False):\n",
    "            y = batch.cuda().float()\n",
    "            # Forward pass\n",
    "            loss_dict, _ = rectified_flows(model, y)\n",
    "            loss = loss_dict['loss']\n",
    "            val_total_loss += loss.item()\n",
    "    avg_val_loss = val_total_loss / len(dataloader_val)\n",
    "    print(f'Epoch {epoch + 1}, Validation Loss: {avg_val_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54134c7e0f0b4381"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Test the Model\n",
    "# Generate samples\n",
    "num_samples = 5\n",
    "num_steps = 100  # Number of diffusion steps\n",
    "os.makedirs('generated_audio', exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for i in range(num_samples):\n",
    "        # Get latents_shape from dataset's mean\n",
    "        latents_shape = (1, *dataset.mean.shape, 32)  # (batch_size=1, channels, seq_len)\n",
    "        generated_latents = inference(rectified_flows, model, latents_shape, num_steps)\n",
    "        # Unnormalize the generated latents\n",
    "        mean_device = dataset.mean.to(generated_latents.device)\n",
    "        std_device = dataset.std.to(generated_latents.device)\n",
    "        generated_latents = generated_latents.squeeze(0)  # Remove batch dimension\n",
    "        generated_latents = (generated_latents.permute(1, 0) * std_device + mean_device).permute(1, 0)\n",
    "        # Decode the generated latents into audio\n",
    "        generated_latents_np = generated_latents.cpu().numpy()  # Shape: [channels, seq_len]\n",
    "        wv_rec = encdec.decode(generated_latents_np)\n",
    "        # Save the audio file\n",
    "        output_filename = f'generated_audio/diffusion_sample_{i + 1}.wav'\n",
    "        sf.write(output_filename,\n",
    "                 wv_rec[0],\n",
    "                 samplerate=44100)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd2a1ae0193150ed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
